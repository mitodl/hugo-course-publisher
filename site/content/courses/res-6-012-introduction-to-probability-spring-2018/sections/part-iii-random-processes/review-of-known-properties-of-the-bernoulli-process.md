---
about_this_resource_text: '<p><strong>Instructor:</strong> John Tsitsiklis</p>'
embedded_media:
  - id: Video-YouTube-Stream
    media_location: GwOklYjwHDI
    parent_uid: 0e7d44aedf04a775b674f98c8578ac8f
    title: Video-YouTube-Stream
    type: Video
    uid: fdb2ccad2c3231302df4439cebdaf832
  - id: Thumbnail-YouTube-JPG
    media_location: 'https://img.youtube.com/vi/GwOklYjwHDI/default.jpg'
    parent_uid: 0e7d44aedf04a775b674f98c8578ac8f
    title: Thumbnail-YouTube-JPG
    type: Thumbnail
    uid: a8e107a535382989a483dad9fe250a21
  - id: 3Play-3PlayYouTubeid-MP4
    media_location: GwOklYjwHDI
    parent_uid: 0e7d44aedf04a775b674f98c8578ac8f
    title: 3Play-3Play YouTube id
    type: 3Play
    uid: 5e851bfd18c9de226f2d7e9ae3ad4e6d
  - id: GwOklYjwHDI.srt
    parent_uid: 0e7d44aedf04a775b674f98c8578ac8f
    technical_location: >-
      https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-iii-random-processes/review-of-known-properties-of-the-bernoulli-process/GwOklYjwHDI.srt
    title: 3play caption file
    type: null
    uid: 28fd10e75587773febbff575e9087ac8
  - id: GwOklYjwHDI.pdf
    parent_uid: 0e7d44aedf04a775b674f98c8578ac8f
    technical_location: >-
      https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-iii-random-processes/review-of-known-properties-of-the-bernoulli-process/GwOklYjwHDI.pdf
    title: 3play pdf file
    type: null
    uid: 9fb71e37908e232dfef9917087e1d5a0
  - id: Caption-3Play YouTube id-SRT
    parent_uid: 0e7d44aedf04a775b674f98c8578ac8f
    title: Caption-3Play YouTube id-SRT-English - US
    type: Caption
    uid: ca292abd4b18c3e7f5b817dfa9d6eb5d
  - id: Transcript-3Play YouTube id-PDF
    parent_uid: 0e7d44aedf04a775b674f98c8578ac8f
    title: Transcript-3Play YouTube id-PDF-English - US
    type: Transcript
    uid: 7889f27b00d30c99b59b6cec5dad7cca
  - id: Video-InternetArchive-MP4
    media_location: >-
      https://archive.org/download/MITRES.6-012S18/MITRES6_012S18_L21-04_300k.mp4
    parent_uid: 0e7d44aedf04a775b674f98c8578ac8f
    title: Video-Internet Archive-MP4
    type: Video
    uid: 31de613159556fe4148a3f09eab45994
inline_embed_id: 76909098reviewofknownpropertiesofthebernoulliprocess3020902
order_index: 1952
parent_uid: ea0e960c7d6bb5ec3c28c2657fe85c0d
related_resources_text: ''
short_url: review-of-known-properties-of-the-bernoulli-process
technical_location: >-
  https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-iii-random-processes/review-of-known-properties-of-the-bernoulli-process
title: " \tReview of Known Properties of the Bernoulli Process"
transcript: >-
  <p><span m='740'>In this segment, we go through a quick review</span> <span
  m='3350'>of a few properties of the Bernoulli process</span> <span
  m='5700'>that we already know.</span> </p><p><span m='7420'>We start by
  thinking about the number of successes or arrivals</span> <span m='11410'>in
  the first n time slots.</span> </p><p><span m='13740'>This is the following
  quantity.</span> </p><p><span m='16270'>At each time we add a 0 or a 1,
  depending on</span> <span m='20300'>whether we've had a success or not,</span>
  <span m='22650'>then by adding those numbers, we get the total number</span>
  <span m='25150'>of successes.</span> </p><p><span m='26690'>Now we already
  know that the number of successes in n trials</span> <span m='31080'>obeys a
  binomial distribution, so the probability</span> <span m='34410'>of having k
  successes is given by the binomial probabilities.</span> </p><p><span
  m='40320'>And this is a formula that holds for k equal to 0 up to n,</span>
  <span m='46380'>which are the possible numbers for the random variable
  S.</span> </p><p><span m='49790'>For this random variable, we know the
  expected value.</span> </p><p><span m='52740'>It's n times p.</span>
  </p><p><span m='54500'>And we also know its variance, which is n times</span>
  <span m='57660'>p times 1 minus p.</span> </p><p><span m='61410'>Another
  random variable of interest</span> <span m='63340'>is the time until the first
  success or arrival.</span> </p><p><span m='67240'>So this is defined to be the
  smallest</span> <span m='70430'>i for which the random variable Xi is equal to
  1.</span> </p><p><span m='79670'>We have done this calculation in the
  past.</span> </p><p><span m='81900'>The probability that the first success
  appears at time k is</span> <span m='87550'>the same as the probability that
  the first k minus 1 trials</span> <span m='94520'>resulted in 0's.</span>
  </p><p><span m='97850'>And then, the k-th trial resulted in a 1.</span>
  </p><p><span m='101530'>And so the probability of this is 1 minus p,</span>
  <span m='106620'>the probability of 0, and we have k minus 1 of them,</span>
  <span m='111070'>times p, the probability that the next trial gives us</span>
  <span m='114870'>a success.</span> </p><p><span m='116080'>And this formula is
  valid for k being 1, 2, and so on,</span> <span m='122300'>which is the range
  of possible values</span> <span m='124850'>of this random variable T1.</span>
  </p><p><span m='127160'>This is the familiar geometric distribution</span>
  <span m='130020'>that we have dealt with on several occasions.</span>
  </p><p><span m='133300'>And in particular, we know the expected value and the
  variance</span> <span m='136930'>of the geometric random variable.</span> </p>
uid: 0e7d44aedf04a775b674f98c8578ac8f
type: course
layout: video
---
