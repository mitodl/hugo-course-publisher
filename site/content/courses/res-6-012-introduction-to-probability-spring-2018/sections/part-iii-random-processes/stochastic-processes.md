---
about_this_resource_text: '<p><strong>Instructor:</strong> John Tsitsiklis</p>'
embedded_media:
  - id: Video-YouTube-Stream
    media_location: JYI5xKlH_MU
    parent_uid: cc7f8c89c56491ad267ed38fac97c699
    title: Video-YouTube-Stream
    type: Video
    uid: 912060cbf550b3de39179efcd47900d0
  - id: Thumbnail-YouTube-JPG
    media_location: 'https://img.youtube.com/vi/JYI5xKlH_MU/default.jpg'
    parent_uid: cc7f8c89c56491ad267ed38fac97c699
    title: Thumbnail-YouTube-JPG
    type: Thumbnail
    uid: 7a45f9d38dce03c244ad203db9a9d6ae
  - id: 3Play-3PlayYouTubeid-MP4
    media_location: JYI5xKlH_MU
    parent_uid: cc7f8c89c56491ad267ed38fac97c699
    title: 3Play-3Play YouTube id
    type: 3Play
    uid: 94f940998281072111e93302312b6ea4
  - id: JYI5xKlH_MU.srt
    parent_uid: cc7f8c89c56491ad267ed38fac97c699
    technical_location: >-
      https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-iii-random-processes/stochastic-processes/JYI5xKlH_MU.srt
    title: 3play caption file
    type: null
    uid: 6e9d06474e7b71928e403da0c3e11197
  - id: JYI5xKlH_MU.pdf
    parent_uid: cc7f8c89c56491ad267ed38fac97c699
    technical_location: >-
      https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-iii-random-processes/stochastic-processes/JYI5xKlH_MU.pdf
    title: 3play pdf file
    type: null
    uid: be33f429d9b97b2c65f1e82b8f8d937f
  - id: Caption-3Play YouTube id-SRT
    parent_uid: cc7f8c89c56491ad267ed38fac97c699
    title: Caption-3Play YouTube id-SRT-English - US
    type: Caption
    uid: c04dfd86d2f704496bcc2461efaccc7d
  - id: Transcript-3Play YouTube id-PDF
    parent_uid: cc7f8c89c56491ad267ed38fac97c699
    title: Transcript-3Play YouTube id-PDF-English - US
    type: Transcript
    uid: 791b73a6337f5c3cbc69efea294d3af7
  - id: Video-InternetArchive-MP4
    media_location: >-
      https://archive.org/download/MITRES.6-012S18/MITRES6_012S18_L21-03_300k.mp4
    parent_uid: cc7f8c89c56491ad267ed38fac97c699
    title: Video-Internet Archive-MP4
    type: Video
    uid: 7661f84504020ce98d347ac4902b8f64
inline_embed_id: 69861621stochasticprocesses98317794
order_index: 1943
parent_uid: ea0e960c7d6bb5ec3c28c2657fe85c0d
related_resources_text: ''
short_url: stochastic-processes
technical_location: >-
  https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-iii-random-processes/stochastic-processes
title: Stochastic Processes
transcript: >-
  <p><span m='670'>We have said that the Bernoulli process is the
  simplest</span> <span m='3480'>stochastic processes there is.</span>
  </p><p><span m='5520'>But what is a stochastic process anyway?</span>
  </p><p><span m='8490'>A stochastic process can be thought</span> <span
  m='10515'>of as a sequence of random variables.</span> </p><p><span
  m='14140'>Now, how is this different from what we have doing before,</span>
  <span m='16800'>where we have dealt with multiple random variables?</span>
  </p><p><span m='20390'>Well, one difference is that here we're</span> <span
  m='22560'>talking about an infinite sequence of random variables.</span>
  </p><p><span m='26810'>And that complicates things to a certain extent.</span>
  </p><p><span m='29980'>Now, what does it take to describe a stochastic
  process?</span> </p><p><span m='33740'>We should specify the properties of
  each one</span> <span m='36980'>of those random variables.</span> </p><p><span
  m='38620'>For example, we might be interested in the mean,</span> <span
  m='40810'>variance, or PMF of those random variables.</span> </p><p><span
  m='43820'>For the case of the Bernoulli process,</span> <span m='45580'>this
  would be easy to do.</span> </p><p><span m='46920'>We know what the expected
  value is.</span> </p><p><span m='48940'>We have a formula for the
  variance.</span> </p><p><span m='51340'>And we have a fairly simple
  PMF.</span> </p><p><span m='54700'>There's probability p that X is equal to 1
  and probability 1</span> <span m='58580'>minus p that X equals to 0.</span>
  </p><p><span m='62190'>But this is not enough.</span> </p><p><span
  m='64069'>We also need to know how the different random variables are</span>
  <span m='66930'>related to each other.</span> </p><p><span m='68640'>And this
  is done by specifying, directly or indirectly,</span> <span m='72900'>the
  joint distribution, the joint PMF or PDF,</span> <span m='76990'>of the random
  variables involved.</span> </p><p><span m='79390'>And because we have an
  infinite number of random variables,</span> <span m='82990'>it's not enough to
  do this, let's say,</span> <span m='85100'>for the first n of them.</span>
  </p><p><span m='86850'>We need to be able to specify this joint
  distribution</span> <span m='90360'>no matter what the number n is.</span>
  </p><p><span m='94410'>For the case of the Bernoulli process,</span> <span
  m='96470'>we have specified this joint PMF in an indirect way,</span> <span
  m='100690'>because we have said that the random variables are</span> <span
  m='103539'>independent of each other.</span> </p><p><span m='105450'>So the
  joint factors as a product of the marginals.</span> </p><p><span
  m='111210'>And we already know what the marginals are.</span> </p><p><span
  m='114830'>So we do, indeed, have a specification of the joint PMF,</span>
  <span m='118830'>and we have that for all values of n.</span> </p><p><span
  m='121460'>Of course, for more complicated stochastic processes,</span> <span
  m='124430'>this calculation might be somewhat more difficult.</span>
  </p><p><span m='129100'>Now, there is a second view of a stochastic
  process</span> <span m='132040'>which rests on the following.</span>
  </p><p><span m='133840'>It's not just a collection of random variables,</span>
  <span m='136340'>but they are a collection that's indexed</span> <span
  m='138620'>by an index that keeps increasing.</span> </p><p><span
  m='141090'>And quite often, we think of this index as corresponding</span>
  <span m='144070'>to time.</span> </p><p><span m='145530'>And so we have a
  mental picture that</span> <span m='147380'>involves a process that keeps
  evolving in time.</span> </p><p><span m='151530'>What is this picture?</span>
  </p><p><span m='152810'>This picture is best developed if we</span> <span
  m='154470'>think in terms of the sample space.</span> </p><p><span
  m='156920'>Although we have an infinite sequence of random variables,</span>
  <span m='160440'>we are dealing with a single experiment.</span> </p><p><span
  m='163190'>And that single experiment runs in time.</span> </p><p><span
  m='166329'>And when we carry out the experiment,</span> <span m='168610'>we
  might to get an outcome such as the following.</span> </p><p><span
  m='171970'>For the Bernoulli process, we might get a 0, 0, 1, 0, 1, 1,</span>
  <span m='177879'>0, and so on.</span> </p><p><span m='179510'>And we
  continue.</span> </p><p><span m='181100'>So an infinite sequence of that
  kind</span> <span m='183640'>is one possible outcome of this infinitely long
  experiment,</span> <span m='189400'>one particular outcome of the stochastic
  process.</span> </p><p><span m='193200'>If we carry out the process once
  more,</span> <span m='196310'>we might get a different outcome.</span>
  </p><p><span m='198590'>For example, we might get a 0, 1, 1, 0, 0, 0, 1, 1,
  and so on,</span> <span m='205550'>and continuing.</span> </p><p><span
  m='206810'>And in general, any time function of this kind</span> <span
  m='210760'>is one possible outcome of the experiment.</span> </p><p><span
  m='214430'>Overall, the sample space that we're dealing with</span> <span
  m='218300'>is the set of all infinite sequences of 0s and 1s.</span>
  </p><p><span m='232770'>This point of view emphasizes the fact</span> <span
  m='234810'>that we have a phenomenon that evolves over time</span> <span
  m='238910'>and can be used to answer questions that</span> <span
  m='241650'>have to do with the long-term evolution of this process.</span>
  </p><p><span m='245650'>Here's one particular kind of question</span> <span
  m='247310'>we might want one answer.</span> </p><p><span m='249340'>What is
  the probability that all of the Xi's turn out to be 1?</span> </p><p><span
  m='254010'>Notice that this is an event that involves all of the Xi's</span>
  <span m='258079'>not just a finite number of them.</span> </p><p><span
  m='260630'>So this is not a probability that we</span> <span m='262650'>can
  calculate right away by using this joint pmf.</span> </p><p><span
  m='266380'>We need to do a little more work.</span> </p><p><span
  m='269100'>What is the work that we want to do?</span> </p><p><span
  m='271600'>Instead of calculating this quantity,</span> <span m='274010'>we
  will calculate a somewhat related quantity.</span> </p><p><span m='276900'>Let
  us look at the event that the first n</span> <span m='281159'>results were
  equal to 1.</span> </p><p><span m='284180'>How is this event related to this
  event?</span> </p><p><span m='287810'>Well, this event here implies that this
  event has happened.</span> </p><p><span m='292920'>So this is a smaller
  event.</span> </p><p><span m='295050'>This is more difficult to obtain than
  this one.</span> </p><p><span m='298430'>And this gives us an inequality for
  the probabilities</span> <span m='301810'>that go this way.</span>
  </p><p><span m='303380'>Now, we know that this probability</span> <span
  m='305160'>is equal to p to the n.</span> </p><p><span m='307630'>And this
  inequality here is true for all n.</span> </p><p><span m='312940'>No matter
  how large n we take, this quantity</span> <span m='316550'>is smaller than
  that.</span> </p><p><span m='318470'>But now, since p has been assumed to be
  less than 1,</span> <span m='326120'>when we take n larger and larger,</span>
  <span m='328190'>this number becomes arbitrarily small.</span> </p><p><span
  m='330570'>So this quantity is less than or equal</span> <span m='332480'>to
  an arbitrarily small number.</span> </p><p><span m='334430'>So this quantity
  can only be equal to 0.</span> </p><p><span m='338400'>And this is a simple
  example of how we calculate properties</span> <span m='342900'>of the
  stochastic process as it evolves over the infinite time</span> <span
  m='347710'>horizon and how we can sometimes calculate them using</span> <span
  m='352620'>these so-called finite dimensional joint probabilities</span> <span
  m='357090'>that tell us what the process is doing</span> <span m='359100'>over
  a finite amount of time.</span> </p><p><span m='363130'>Throughout, we will
  sometimes view stochastic processes</span> <span m='366670'>in this manner, in
  terms of probability distributions.</span> </p><p><span m='370090'>But
  sometimes we will also want to reason</span> <span m='373010'>in terms of the
  behavior of the stochastic process</span> <span m='375880'>as a time function,
  as a process that evolves in time.</span> </p>
uid: cc7f8c89c56491ad267ed38fac97c699
type: course
layout: video
---
