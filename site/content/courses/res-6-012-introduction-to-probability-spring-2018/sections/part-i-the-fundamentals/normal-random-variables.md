---
about_this_resource_text: '<p><strong>Instructor:</strong> John Tsitsiklis</p>'
embedded_media:
  - id: Video-YouTube-Stream
    media_location: 6UMv4vb4y7c
    parent_uid: 986ce5282b8d9a69b4e8de8d1a3f7718
    title: Video-YouTube-Stream
    type: Video
    uid: 0b116f53bee3b4a1a55132832510e25a
  - id: Thumbnail-YouTube-JPG
    media_location: 'https://img.youtube.com/vi/6UMv4vb4y7c/default.jpg'
    parent_uid: 986ce5282b8d9a69b4e8de8d1a3f7718
    title: Thumbnail-YouTube-JPG
    type: Thumbnail
    uid: c613cd332478fe6df12cea997a21f93e
  - id: 3Play-3PlayYouTubeid-MP4
    media_location: 6UMv4vb4y7c
    parent_uid: 986ce5282b8d9a69b4e8de8d1a3f7718
    title: 3Play-3Play YouTube id
    type: 3Play
    uid: d90b2870eea09bd6bd8c46b6b07e0514
  - id: 6UMv4vb4y7c.srt
    parent_uid: 986ce5282b8d9a69b4e8de8d1a3f7718
    technical_location: >-
      https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-i-the-fundamentals/normal-random-variables/6UMv4vb4y7c.srt
    title: 3play caption file
    type: null
    uid: 8fc783b66e82e5f621ac350a4747198a
  - id: 6UMv4vb4y7c.pdf
    parent_uid: 986ce5282b8d9a69b4e8de8d1a3f7718
    technical_location: >-
      https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-i-the-fundamentals/normal-random-variables/6UMv4vb4y7c.pdf
    title: 3play pdf file
    type: null
    uid: 48bd099eaa209d8714acd230066a30d1
  - id: Caption-3Play YouTube id-SRT
    parent_uid: 986ce5282b8d9a69b4e8de8d1a3f7718
    title: Caption-3Play YouTube id-SRT-English - US
    type: Caption
    uid: c111245b1bba88393ce325d65a42a226
  - id: Transcript-3Play YouTube id-PDF
    parent_uid: 986ce5282b8d9a69b4e8de8d1a3f7718
    title: Transcript-3Play YouTube id-PDF-English - US
    type: Transcript
    uid: 89232a9d66541267b8520253de8a5cf9
  - id: Video-InternetArchive-MP4
    media_location: >-
      https://archive.org/download/MITRES.6-012S18/MITRES6_012S18_L08-08_300k.mp4
    parent_uid: 986ce5282b8d9a69b4e8de8d1a3f7718
    title: Video-Internet Archive-MP4
    type: Video
    uid: 3baab439acb45d9ffc7a91f2c6158590
inline_embed_id: 8634659normalrandomvariables51771054
order_index: 807
parent_uid: 9ca6b310dc93095c9ac0f0e5f95e6930
related_resources_text: ''
short_url: normal-random-variables
technical_location: >-
  https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-i-the-fundamentals/normal-random-variables
title: Normal Random Variables
transcript: >-
  <p><span m='1510'>We now introduce normal random variables, which are
  also</span> <span m='5240'>often called Gaussian random variables.</span>
  </p><p><span m='8029'>Normal random variables are perhaps the most
  important</span> <span m='10930'>ones in probability theory.</span>
  </p><p><span m='13070'>They play a key role in the theory of the subject, as
  we</span> <span m='16650'>will see later in this class in the context of the
  central</span> <span m='19860'>limit theorem.</span> </p><p><span
  m='21300'>They're also prevalent in applications for two reasons.</span>
  </p><p><span m='25330'>They have some nice analytical properties, and they're
  are</span> <span m='30120'>also the most common model of random noise.</span>
  </p><p><span m='34470'>In general, they are a good model of noise or
  randomness</span> <span m='37950'>whenever that noise is due to the addition
  of many small</span> <span m='41860'>independent noise terms, and this is a
  very common</span> <span m='45520'>situation in the real world.</span>
  </p><p><span m='50080'>We define normal random variables by specifying
  their</span> <span m='52890'>PDFs, and we start with the simplest case of the
  so-called</span> <span m='56660'>standard normal.</span> </p><p><span
  m='58210'>The standard normal is indicated with this shorthand</span> <span
  m='61370'>notation, and we will see shortly why this notation is</span> <span
  m='66080'>being used.</span> </p><p><span m='67410'>It is defined in terms of
  a PDF.</span> </p><p><span m='69970'>This PDF is defined for all values of x.
  x</span> <span m='73610'>can be any real number.</span> </p><p><span
  m='75750'>So this random variable can take values</span> <span
  m='79100'>anywhere on the real line.</span> </p><p><span m='80900'>And the
  formula for the PDF is this one.</span> </p><p><span m='83630'>Let us try to
  understand this formula.</span> </p><p><span m='86820'>So we have the
  exponential of negative x squared over 2.</span> </p><p><span m='91660'>Now,
  if we are to plot the x squared over 2 function, it</span> <span m='97229'>has
  a shape of this form, and it is centered at zero.</span> </p><p><span
  m='106100'>But then we take the negative exponential of this function.</span>
  </p><p><span m='110700'>Now, when you take the negative exponential,
  whenever</span> <span m='114360'>this thing is big, the negative exponential
  is going</span> <span m='117410'>to be small.</span> </p><p><span
  m='118530'>So the negative exponential would be equal to 1 when x is</span>
  <span m='123170'>equal to 0.</span> </p><p><span m='124470'>But then as x
  increases, because x squared also</span> <span m='128690'>increases, the
  negative exponential will fall off.</span> </p><p><span m='132170'>And so we
  obtain a shape of this kind, and symmetrically</span> <span m='137060'>on the
  other side as well.</span> </p><p><span m='141220'>And finally, there is this
  constant.</span> </p><p><span m='143850'>Where [is] this constant coming
  from?</span> </p><p><span m='146570'>Well there's a nice and not completely
  straightforward</span> <span m='151050'>calculus exercise that tells us that
  the integral from</span> <span m='155430'>minus infinity to plus infinity of e
  to the negative</span> <span m='159350'>x squared over 2, dx, is equal to the
  square root of 2 pi.</span> </p><p><span m='167860'>Now, we need a PDF to
  integrates to 1.</span> </p><p><span m='171210'>And so for this to happen,
  this is the constant that we</span> <span m='175360'>need to put in front of
  this expression so that the</span> <span m='178630'>integral becomes 1, and
  that explains the presence of this</span> <span m='182330'>particular
  constant.</span> </p><p><span m='184570'>What is the mean of this random
  variable?</span> </p><p><span m='188260'>Well, x squared is symmetric around
  0, and for this reason,</span> <span m='192280'>the PDF itself is symmetric
  around 0.</span> </p><p><span m='195740'>And therefore, by symmetry, the mean
  has to be equal to 0.</span> </p><p><span m='201800'>And that explains this
  entry here.</span> </p><p><span m='204540'>How about the variance?</span>
  </p><p><span m='206430'>Well, to calculate the variance, you need to solve
  a</span> <span m='210150'>calculus problem again.</span> </p><p><span
  m='211930'>You need to integrate by parts.</span> </p><p><span m='216890'>And
  after you carry out the calculation, then you find</span> <span
  m='221380'>that the variance is equal to 1, and that explains this</span>
  <span m='226090'>entry here in the notation that we have been using.</span>
  </p><p><span m='230800'>Let us now define general normal random
  variables.</span> </p><p><span m='234329'>General normal random variables are
  once more</span> <span m='237070'>specified in terms of the corresponding PDF,
  but this</span> <span m='241440'>PDF is a little more complicated, and it
  involves</span> <span m='244810'>two parameters--</span> <span m='246190'>mu
  and sigma squared, where sigma is a</span> <span m='250950'>given positive
  parameter.</span> </p><p><span m='253760'>Once more, it will have a bell
  shape, but this bell is no</span> <span m='259050'>longer symmetric around 0,
  and there is some control over the</span> <span m='263150'>width of it.</span>
  </p><p><span m='265060'>Let us understand the form of this PDF by focusing
  first on</span> <span m='269610'>the exponent, exactly as we did for
  the</span> <span m='271710'>standard normal case.</span> </p><p><span
  m='274060'>The exponent is a quadratic, and that quadratic is centered</span>
  <span m='282416'>at x equal to mu.</span> </p><p><span m='286290'>So it
  vanishes when x is equal to mu, and</span> <span m='289750'>becomes positive
  elsewhere.</span> </p><p><span m='291980'>Then we take the negative
  exponential of this quadratic,</span> <span m='295340'>and we obtain a
  function which is largest at x equal to mu,</span> <span m='300640'>and falls
  off as we go further away from mu.</span> </p><p><span m='308530'>What is the
  mean of this random variable?</span> </p><p><span m='311880'>Since this term
  is symmetric around mu, the PDF is also</span> <span m='316550'>symmetric
  around mu, and therefore, the mean is also</span> <span m='320770'>equal to
  mu.</span> </p><p><span m='322660'>How about the variance?</span> </p><p><span
  m='324500'>It turns out--</span> <span m='325780'>and this is a calculus
  exercise that we will omit--</span> <span m='328620'>that the variance of this
  PDF is equal to sigma squared.</span> </p><p><span m='332800'>And this
  explains this notation here.</span> </p><p><span m='334909'>We're dealing with
  a normal that has a mean of mu and a</span> <span m='337610'>variance of sigma
  squared.</span> </p><p><span m='339700'>To get a little bit of understanding
  of the role of</span> <span m='343380'>sigma in the form of this PDF, let us
  consider the case where</span> <span m='348930'>sigma is small, and see how
  the</span> <span m='352680'>picture is going to change.</span> </p><p><span
  m='354950'>When sigma is small, and we plot the quadratic, sigma</span> <span
  m='359890'>being small means that this quadratic becomes larger, so</span>
  <span m='365260'>it rises faster, so we get a narrower quadratic.</span>
  </p><p><span m='370290'>And in that case, the negative exponential is going to
  fall</span> <span m='375340'>off much faster.</span> </p><p><span
  m='378920'>So when sigma is small, the PDF that we get is a narrower</span>
  <span m='385270'>PDF, and that reflects itself into the property that
  the</span> <span m='391370'>variance will also be smaller.</span> </p><p><span
  m='397710'>An important property of normal random variables is</span> <span
  m='400650'>that they behave very nicely when you form linear</span> <span
  m='403690'>functions of them.</span> </p><p><span m='405090'>And this is one
  of the reasons why they're analytically</span> <span m='408280'>tractable and
  analytically very convenient.</span> </p><p><span m='411450'>Here is what I
  mean.</span> </p><p><span m='412890'>Let us start with a normal random
  variable with a given</span> <span m='415620'>mean and variance, and let us
  form a linear function of that</span> <span m='418810'>random variable.</span>
  </p><p><span m='420330'>What is the mean of Y?</span> </p><p><span
  m='422170'>Well, we know what it is.</span> </p><p><span m='425060'>We have a
  linear function of a random variable.</span> </p><p><span m='427280'>The mean
  is going to be a times the expected value of X,</span> <span m='430600'>which
  is mu plus b.</span> </p><p><span m='433700'>What is the variance of Y?</span>
  </p><p><span m='436270'>We know what is the variance of a linear function of
  a</span> <span m='439030'>random variable.</span> </p><p><span m='439750'>It
  is a squared times the variance of X, which in our</span> <span
  m='443570'>case is sigma squared.</span> </p><p><span m='445700'>So there's
  nothing new so far, but there is an additional</span> <span
  m='449130'>important fact.</span> </p><p><span m='450435'>The random variable
  Y, of course, has the mean and</span> <span m='454590'>variance that we know
  it should have, but there is an</span> <span m='458080'>additional
  fact--</span> <span m='459500'>namely, that Y is a normal random
  variable.</span> </p><p><span m='464150'>So normality is preserved when we
  form linear functions.</span> </p><p><span m='470080'>There's one special case
  that's we need to pay some</span> <span m='472440'>attention to.</span>
  </p><p><span m='473780'>Suppose that a is equal to 0.</span> </p><p><span
  m='476750'>In this case, the random variable Y is just equal to b.</span>
  </p><p><span m='481250'>It's a constant random variable.</span> </p><p><span
  m='483430'>It does not have a PDF.</span> </p><p><span m='485840'>It is a
  degenerate discrete random variable.</span> </p><p><span m='490790'>So could
  this fact be correct that Y is also normal?</span> </p><p><span
  m='496000'>Well, we'll adopt this as [a] convention.</span> </p><p><span
  m='499980'>When we have a discrete random variable, which is constant,</span>
  <span m='505420'>it takes a constant value.</span> </p><p><span m='507080'>We
  can think of this as a special degenerate case of the</span> <span
  m='510630'>normal with mean equal to b and with variance equal to 0.</span>
  </p><p><span m='517179'>Even though it is discrete, not continuous, we will
  still</span> <span m='522070'>think of it as a degenerate type of a normal
  random</span> <span m='525690'>variable, and by adopting this convention, then
  it will</span> <span m='529240'>always be true that a linear function of a
  normal random</span> <span m='532640'>variable is normal, even if a is equal
  to 0.</span> </p><p><span m='538710'>Now that we have the definition and some
  properties</span> <span m='541080'>of normal random variables, the next
  question is whether</span> <span m='544140'>we can calculate probabilities
  associated with</span> <span m='547730'>normal random variables.</span>
  </p><p><span m='549410'>This will be the subject of the next segment.</span>
  </p><p></p>
uid: 986ce5282b8d9a69b4e8de8d1a3f7718
type: course
layout: video
---
