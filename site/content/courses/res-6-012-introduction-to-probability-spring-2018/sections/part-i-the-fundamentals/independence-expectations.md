---
about_this_resource_text: '<p><strong>Instructor:</strong> John Tsitsiklis</p>'
embedded_media:
  - id: Video-YouTube-Stream
    media_location: R4nGGs0m7lo
    parent_uid: 0e8be2bea6c608f6d162c352f0c08d97
    title: Video-YouTube-Stream
    type: Video
    uid: c12d121214ecc8e39337a55898c7ebc5
  - id: Thumbnail-YouTube-JPG
    media_location: 'https://img.youtube.com/vi/R4nGGs0m7lo/default.jpg'
    parent_uid: 0e8be2bea6c608f6d162c352f0c08d97
    title: Thumbnail-YouTube-JPG
    type: Thumbnail
    uid: d2ee6db4c1138d6527428f22a16288a7
  - id: 3Play-3PlayYouTubeid-MP4
    media_location: R4nGGs0m7lo
    parent_uid: 0e8be2bea6c608f6d162c352f0c08d97
    title: 3Play-3Play YouTube id
    type: 3Play
    uid: 2f1d9629bc939e7e2e34a03a4c238ef8
  - id: R4nGGs0m7lo.srt
    parent_uid: 0e8be2bea6c608f6d162c352f0c08d97
    technical_location: >-
      https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-i-the-fundamentals/independence-expectations/R4nGGs0m7lo.srt
    title: 3play caption file
    type: null
    uid: a52ce31a437e6550019b4169a5500aa5
  - id: R4nGGs0m7lo.pdf
    parent_uid: 0e8be2bea6c608f6d162c352f0c08d97
    technical_location: >-
      https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-i-the-fundamentals/independence-expectations/R4nGGs0m7lo.pdf
    title: 3play pdf file
    type: null
    uid: c43453567591ad07e9eed6c9be07b847
  - id: Caption-3Play YouTube id-SRT
    parent_uid: 0e8be2bea6c608f6d162c352f0c08d97
    title: Caption-3Play YouTube id-SRT-English - US
    type: Caption
    uid: eeac93694de71f8cb728ccf34143014c
  - id: Transcript-3Play YouTube id-PDF
    parent_uid: 0e8be2bea6c608f6d162c352f0c08d97
    title: Transcript-3Play YouTube id-PDF-English - US
    type: Transcript
    uid: 8937a72422f9c179fca835404591bd2c
  - id: Video-InternetArchive-MP4
    media_location: >-
      https://archive.org/download/MITRES.6-012S18/MITRES6_012S18_L07-06_300k.mp4
    parent_uid: 0e8be2bea6c608f6d162c352f0c08d97
    title: Video-Internet Archive-MP4
    type: Video
    uid: 5ae506a7137a716bca3fdce0c2b94a93
inline_embed_id: 185732independenceexpectations43888099
order_index: 690
parent_uid: 9ca6b310dc93095c9ac0f0e5f95e6930
related_resources_text: ''
short_url: independence-expectations
technical_location: >-
  https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-i-the-fundamentals/independence-expectations
title: Independence & Expectations
transcript: >-
  <p><span m="270">When we have independence, does anything interesting</span>
  <span m="3330">happen to expectations?</span></p><p><span m="5640">We know
  that, in general, the expected value of a function</span> <span m="8400">of
  random variables is not the same as applying the function</span> <span
  m="12180">to the expected values.</span></p><p><span m="14430">And we also
  know that there are some exceptions where we</span> <span m="18820">do get
  equality.</span></p><p><span m="20770">This is the case where we are dealing
  with linear functions</span> <span m="23820">of one or more random
  variables.</span></p><p><span m="28040">Note that this last property is always
  true and does not</span> <span m="34980">require any independence
  assumptions.</span></p><p><span m="39160">When we have independence, there is
  one additional</span> <span m="42070">property that turns out to be
  true.</span></p><p><span m="45490">The expected value of the product of two
  independent</span> <span m="49650">random variables is the product of</span>
  <span m="52340">their expected values.</span></p><p><span m="55360">Let us
  verify this relation.</span></p><p><span m="58580">We are dealing here with
  the expected value of a function</span> <span m="62160">of random variables,
  where the function is defined to be the</span> <span m="69289">product
  function.</span></p><p><span m="73300">So to calculate this expected value,
  you can use the</span> <span m="78660">expected value rule.</span></p><p><span
  m="82210">And we are going to get the sum over all x, the sum over</span>
  <span m="87580">all y, of g of xy, but in this case, g of xy is x times
  y.</span></p><p><span m="94930">And then we weigh all those values according
  to the</span> <span m="99740">probabilities as given by the joint
  PMF.</span></p><p><span m="104039">Now, using independence, this sum can be
  changed into the</span> <span m="113930">following form--</span> <span
  m="116440">the joint PMF is the product of the marginal
  PMFs.</span></p><p><span m="123120">And now when we look at the inner sum over
  all values of</span> <span m="126250">y, we can take outside the summation
  those terms that do</span> <span m="132010">not depend on y, and so this term
  and that term.</span></p><p><span m="139260">And this is going to yield a
  summation over x of x times</span> <span m="148680">the marginal PMF of X, and
  then the summation over all y</span> <span m="154650">of y times the marginal
  PMF of Y. But now we recognize that</span> <span m="159630">here we have just
  the expected value of Y. And then we will</span> <span m="166230">be left with
  another expression, which is the</span> <span m="168880">expected value of X.
  And this completes the argument.</span></p><p><span m="179780">Now, consider a
  function of X and another function of Y. X</span> <span m="188520">and Y are
  independent.</span></p><p><span m="190480">Intuitively, the value of X does
  not give you any new</span> <span m="194560">information about Y, so the value
  of g of X does not to</span> <span m="198740">give you any new information
  about h of Y. So on the basis</span> <span m="203740">of this intuitive
  argument, the functions g of X and h of</span> <span m="208230">Y are also
  independent of each other.</span></p><p><span m="211660">Therefore, we can
  apply the fact that we have already</span> <span m="214670">proved, but with g
  of X in the place of X and h of Y in the</span> <span m="220050">place of Y.
  And this gives us this more general fact that</span> <span m="225030">the
  expected value of the product of two functions of</span> <span
  m="228120">independent random variables is equal to the product of the</span>
  <span m="232270">expectations of these functions.</span></p><p><span
  m="235520">We could also prove this property directly without</span> <span
  m="239180">relying on the intuitive argument.</span></p><p><span m="241690">We
  could just follow the same steps as in this derivation.</span></p><p><span
  m="245970">Wherever there is an X, we would write g of X, and</span> <span
  m="249840">wherever there is a Y, we would write h of Y. And the</span> <span
  m="254130">same algebra would go through, and we would end up with the</span>
  <span m="257100">expected value of g of X times the expected value of h of
  Y.</span></p>
uid: 0e8be2bea6c608f6d162c352f0c08d97
type: course
layout: video
---
