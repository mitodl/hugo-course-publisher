---
about_this_resource_text: '<p><strong>Instructor:</strong> John Tsitsiklis</p>'
embedded_media:
  - id: Video-YouTube-Stream
    media_location: JCQnsPggTp8
    parent_uid: f7d85d61b29f17c940c0c0a2441b7a56
    title: Video-YouTube-Stream
    type: Video
    uid: f3587b18e83a362a284992e9df6f631e
  - id: Thumbnail-YouTube-JPG
    media_location: 'https://img.youtube.com/vi/JCQnsPggTp8/default.jpg'
    parent_uid: f7d85d61b29f17c940c0c0a2441b7a56
    title: Thumbnail-YouTube-JPG
    type: Thumbnail
    uid: 8a310dbf8f471660adf6362c823c1041
  - id: 3Play-3PlayYouTubeid-MP4
    media_location: JCQnsPggTp8
    parent_uid: f7d85d61b29f17c940c0c0a2441b7a56
    title: 3Play-3Play YouTube id
    type: 3Play
    uid: e1556295ed73c88e16ffe975ee3a050f
  - id: JCQnsPggTp8.srt
    parent_uid: f7d85d61b29f17c940c0c0a2441b7a56
    technical_location: >-
      https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-i-the-fundamentals/independence/JCQnsPggTp8.srt
    title: 3play caption file
    type: null
    uid: a4feb2c168554d52c16437db6e0cdf8e
  - id: JCQnsPggTp8.pdf
    parent_uid: f7d85d61b29f17c940c0c0a2441b7a56
    technical_location: >-
      https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-i-the-fundamentals/independence/JCQnsPggTp8.pdf
    title: 3play pdf file
    type: null
    uid: 7b7a216522a9a9c20bc2e0377bb6ef78
  - id: Caption-3Play YouTube id-SRT
    parent_uid: f7d85d61b29f17c940c0c0a2441b7a56
    title: Caption-3Play YouTube id-SRT-English - US
    type: Caption
    uid: f14af7a76640daaf8ad66ae34e0503b1
  - id: Transcript-3Play YouTube id-PDF
    parent_uid: f7d85d61b29f17c940c0c0a2441b7a56
    title: Transcript-3Play YouTube id-PDF-English - US
    type: Transcript
    uid: 33433fad9bd125d77853911a9cfa5121
  - id: Video-InternetArchive-MP4
    media_location: >-
      https://archive.org/download/MITRES.6-012S18/MITRES6_012S18_L10-05_300k.mp4
    parent_uid: f7d85d61b29f17c940c0c0a2441b7a56
    title: Video-Internet Archive-MP4
    type: Video
    uid: bf9ecf12fb963fd5a9677c446414cddf
inline_embed_id: 59033115independence26584298
order_index: 951
parent_uid: 9ca6b310dc93095c9ac0f0e5f95e6930
related_resources_text: ''
short_url: independence
technical_location: >-
  https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-i-the-fundamentals/independence
title: Independence
transcript: >-
  <p><span m='420'>Independence is one of the central concepts of</span> <span
  m='2880'>probability theory, because it allows us to build large</span> <span
  m='5940'>models from simpler ones.</span> </p><p><span m='8730'>How should we
  define independence in</span> <span m='10410'>the continuous case?</span>
  </p><p><span m='12210'>Our guide comes from the discrete definition.</span>
  </p><p><span m='15670'>By analogy with the discrete case, we will say that
  two</span> <span m='19950'>jointly continuous random variables are independent
  if</span> <span m='23510'>the joint PDF is equal to the product of the
  marginal PDFs.</span> </p><p><span m='29120'>We can now compare with the
  multiplication rule, which is</span> <span m='32940'>always true as long as
  the density of Y is positive.</span> </p><p><span m='38520'>So this is always
  true.</span> </p><p><span m='40030'>In the case of independence, this is
  true.</span> </p><p><span m='42780'>So in the case of independence, we must
  have</span> <span m='45300'>that this term is equal to that term, at least
  whenever</span> <span m='50350'>this quantity--</span> <span m='52060'>the
  marginal of Y--</span> <span m='53550'>is positive.</span> </p><p><span
  m='55890'>So to restate it, independence is equivalent to having the</span>
  <span m='61130'>conditional, given Y, be the same as the unconditional
  PDF</span> <span m='66240'>of X. And this has to be true whenever Y has a
  positive</span> <span m='71080'>density so that this quantity is well defined,
  and it also</span> <span m='74580'>has to be true for all xs.</span>
  </p><p><span m='77080'>Now, what does this really mean?</span> </p><p><span
  m='79070'>The conditional PDF, as we have discussed, in terms of</span> <span
  m='83140'>pictures, is a slice of the joint PDF.</span> </p><p><span
  m='87630'>Therefore, independence is the same as requiring that all of</span>
  <span m='92160'>the slices of the joint have the same shape, and it is
  the</span> <span m='97440'>shape of the marginal PDF.</span> </p><p><span
  m='100200'>For a more intuitive interpretation, no matter what</span> <span
  m='103780'>value of Y you observe, the distribution</span> <span m='107090'>of
  X does not change.</span> </p><p><span m='109350'>In this sense, Y does not
  convey any information about</span> <span m='112630'>X. Notice also that this
  definition is symmetric as far</span> <span m='117490'>as X and Y are
  concerned.</span> </p><p><span m='119880'>So by symmetry, when we have
  independence, it also means</span> <span m='123960'>that X does not convey any
  information about Y, and that</span> <span m='128568'>the conditional density
  of Y, given X, has to be the same as</span> <span m='133330'>the unconditional
  density of Y.</span> </p><p><span m='137100'>We can also define independence
  of multiple</span> <span m='139450'>random variables.</span> </p><p><span
  m='140410'>The definition is the obvious one.</span> </p><p><span
  m='142340'>The joint PDF of all the random variables involved must</span>
  <span m='145870'>be equal to the product of the marginal PDFs.</span>
  </p><p><span m='149070'>Intuitively, what that means is that knowing the
  values of</span> <span m='152220'>some of the random variables does not affect
  our beliefs</span> <span m='155380'>about the remaining random
  variables.</span> </p><p><span m='158360'>Finally, let us note some
  consequences of independence,</span> <span m='161220'>which are identical to
  the corresponding properties that</span> <span m='163850'>we had in the
  discrete case, and the proofs are also</span> <span m='166750'>exactly the
  same.</span> </p><p><span m='168110'>So the expectation of the product of
  independent random</span> <span m='171130'>variables is the product of the
  expectations, the variance</span> <span m='174620'>of the sum of independent
  random variables is the sum of</span> <span m='177850'>the variances, and
  functions of independent random</span> <span m='181250'>variables are also
  independent, which, in</span> <span m='183620'>particular, implies, using the
  previous rule, that the</span> <span m='187720'>expected value of a product of
  this kind is going to be the</span> <span m='190740'>product of these
  expectations.</span> </p><p><span m='193830'>So independence of continuous
  random variables is pretty</span> <span m='197970'>much the same as
  independence of discrete random variables</span> <span m='201350'>as far as
  mathematics are concerned, and the intuitive</span> <span m='204670'>content
  of the independence assumption is the same as in</span> <span m='208810'>the
  discrete case.</span> </p><p><span m='209820'>One random variable does not
  provide any</span> <span m='211980'>information about the other.</span>
  </p><p></p>
uid: f7d85d61b29f17c940c0c0a2441b7a56
type: courses
layout: video
---
