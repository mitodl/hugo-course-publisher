---
about_this_resource_text: '<p><strong>Instructor:</strong> John Tsitsiklis</p>'
embedded_media:
  - id: Video-YouTube-Stream
    media_location: n9FTM9f9A6I
    parent_uid: 06e92228f7f5ac9254aa55a1f89c2d67
    title: Video-YouTube-Stream
    type: Video
    uid: d1bb10acaece51b13b92a0833e5208b3
  - id: Thumbnail-YouTube-JPG
    media_location: 'https://img.youtube.com/vi/n9FTM9f9A6I/default.jpg'
    parent_uid: 06e92228f7f5ac9254aa55a1f89c2d67
    title: Thumbnail-YouTube-JPG
    type: Thumbnail
    uid: 0e1e564e9548dca0bbfba2b305d2a235
  - id: 3Play-3PlayYouTubeid-MP4
    media_location: n9FTM9f9A6I
    parent_uid: 06e92228f7f5ac9254aa55a1f89c2d67
    title: 3Play-3Play YouTube id
    type: 3Play
    uid: 7eee97e7e6220a5b9100fe1f4fd13abe
  - id: n9FTM9f9A6I.srt
    parent_uid: 06e92228f7f5ac9254aa55a1f89c2d67
    technical_location: >-
      https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-i-the-fundamentals/lecture-overview-5/n9FTM9f9A6I.srt
    title: 3play caption file
    type: null
    uid: beda1507866c068fd0967823c6afc72c
  - id: n9FTM9f9A6I.pdf
    parent_uid: 06e92228f7f5ac9254aa55a1f89c2d67
    technical_location: >-
      https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-i-the-fundamentals/lecture-overview-5/n9FTM9f9A6I.pdf
    title: 3play pdf file
    type: null
    uid: 285b70bb8181729e34661a8d4802ec06
  - id: Caption-3Play YouTube id-SRT
    parent_uid: 06e92228f7f5ac9254aa55a1f89c2d67
    title: Caption-3Play YouTube id-SRT-English - US
    type: Caption
    uid: 710409d63ba033fc5cba5369543da692
  - id: Transcript-3Play YouTube id-PDF
    parent_uid: 06e92228f7f5ac9254aa55a1f89c2d67
    title: Transcript-3Play YouTube id-PDF-English - US
    type: Transcript
    uid: f0100eea07493997bf6e3548577d07f0
  - id: Video-InternetArchive-MP4
    media_location: >-
      https://archive.org/download/MITRES.6-012S18/MITRES6_012S18_L06-01_300k.mp4
    parent_uid: 06e92228f7f5ac9254aa55a1f89c2d67
    title: Video-Internet Archive-MP4
    type: Video
    uid: 9d92675b3e5587482f3da6102039fc0d
inline_embed_id: 36847980lectureoverview21933629
order_index: 573
parent_uid: 9ca6b310dc93095c9ac0f0e5f95e6930
related_resources_text: ''
short_url: lecture-overview-5
technical_location: >-
  https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-i-the-fundamentals/lecture-overview-5
title: Lecture Overview
transcript: >-
  <p><span m="0">In the previous lecture we introduced random variables,</span>
  <span m="3610">probability mass functions and expectations.</span></p><p><span
  m="7050">In this lecture we continue with the development of</span> <span
  m="10170">various concepts associated with random
  variables.</span></p><p><span m="14190">There will be three main
  parts.</span></p><p><span m="16950">In the first part we define the variance
  of a random</span> <span m="19780">variable, and calculate it for some of
  our</span> <span m="22750">familiar random variables.</span></p><p><span
  m="25150">Basically the variance is a quantity that measures the</span> <span
  m="28050">amount of spread, or the dispersion of a probability</span> <span
  m="31900">mass functions.</span></p><p><span m="33310">In some sense, it
  quantifies the amount of randomness that</span> <span m="36780">is
  present.</span></p><p><span m="38210">Together with the expected value, the
  variance summarizes</span> <span m="41870">crisply some of the qualitative
  properties of the</span> <span m="44890">probability mass
  function.</span></p><p><span m="47840">In the second part we discuss
  conditioning.</span></p><p><span m="51500">Every probabilistic concept or
  result has a conditional</span> <span
  m="54670">counterpart.</span></p><p><span m="55850">And this is true for
  probability mass functions,</span> <span m="58680">expectations and
  variances.</span></p><p><span m="61250">We define these conditional
  counterparts and then develop</span> <span m="64160">the total expectation
  theorem.</span></p><p><span m="66850">This is a powerful tool that extends our
  familiar total</span> <span m="71270">probability theorem and allows us to
  divide and conquer when</span> <span m="75050">we calculate
  expectations.</span></p><p><span m="77820">We then take the opportunity to
  dive deeper into the</span> <span m="81360">properties of geometric random
  variables, and use a trick</span> <span m="85450">based on the total
  expectation theorem to</span> <span m="87990">calculate their
  mean.</span></p><p><span m="91039">In the last part we show how to describe
  probabilistically</span> <span m="95300">the relation between multiple random
  variables.</span></p><p><span m="99520">This is done through a so-called joint
  probability</span> <span m="102570">mass function.</span></p><p><span
  m="104360">We take the occasion to generalize the expected value</span> <span
  m="107380">rule, and establish a further linearity property of</span> <span
  m="111280">expectations.</span></p><p><span m="112990">We finally illustrate
  the power of these tools through</span> <span m="116650">the calculation of
  the expected value of a binomial</span> <span m="120060">random
  variable.</span></p><p>&nbsp;</p>
uid: 06e92228f7f5ac9254aa55a1f89c2d67
type: courses
layout: video
---
