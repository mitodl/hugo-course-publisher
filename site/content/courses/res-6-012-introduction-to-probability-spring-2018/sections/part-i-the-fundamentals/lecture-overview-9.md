---
about_this_resource_text: '<p><strong>Instructor:</strong> John Tsitsiklis</p>'
embedded_media:
  - id: Video-YouTube-Stream
    media_location: nQukfQgIIqw
    parent_uid: 4ceeb1fd8ebd21c8c473cf3c336324f5
    title: Video-YouTube-Stream
    type: Video
    uid: f891297282f18064bcaf129f4c55f7d6
  - id: Thumbnail-YouTube-JPG
    media_location: 'https://img.youtube.com/vi/nQukfQgIIqw/default.jpg'
    parent_uid: 4ceeb1fd8ebd21c8c473cf3c336324f5
    title: Thumbnail-YouTube-JPG
    type: Thumbnail
    uid: 112f9d2c5cad58eb58ffb625a9a0d2ac
  - id: 3Play-3PlayYouTubeid-MP4
    media_location: nQukfQgIIqw
    parent_uid: 4ceeb1fd8ebd21c8c473cf3c336324f5
    title: 3Play-3Play YouTube id
    type: 3Play
    uid: 00e3ba16392a050cb35d29b20fdf0626
  - id: nQukfQgIIqw.srt
    parent_uid: 4ceeb1fd8ebd21c8c473cf3c336324f5
    technical_location: >-
      https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-i-the-fundamentals/lecture-overview-9/nQukfQgIIqw.srt
    title: 3play caption file
    type: null
    uid: 4162a0661d83f65a31e1dece6874fab8
  - id: nQukfQgIIqw.pdf
    parent_uid: 4ceeb1fd8ebd21c8c473cf3c336324f5
    technical_location: >-
      https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-i-the-fundamentals/lecture-overview-9/nQukfQgIIqw.pdf
    title: 3play pdf file
    type: null
    uid: 2843d78d99fbb4f643bede8b9ece0eb7
  - id: Caption-3Play YouTube id-SRT
    parent_uid: 4ceeb1fd8ebd21c8c473cf3c336324f5
    title: Caption-3Play YouTube id-SRT-English - US
    type: Caption
    uid: 2179a316ed7fff28b5255a330fd907af
  - id: Transcript-3Play YouTube id-PDF
    parent_uid: 4ceeb1fd8ebd21c8c473cf3c336324f5
    title: Transcript-3Play YouTube id-PDF-English - US
    type: Transcript
    uid: a3f7fc34a11c34493ad6d0e8aafb22d8
  - id: Video-InternetArchive-MP4
    media_location: >-
      https://archive.org/download/MITRES.6-012S18/MITRES6_012S18_L10-01_300k.mp4
    parent_uid: 4ceeb1fd8ebd21c8c473cf3c336324f5
    title: Video-Internet Archive-MP4
    type: Video
    uid: ab77f6df1763d4c108361ea7e6ff02c5
inline_embed_id: 53931333lectureoverview3732639
order_index: 915
parent_uid: 9ca6b310dc93095c9ac0f0e5f95e6930
related_resources_text: ''
short_url: lecture-overview-9
technical_location: >-
  https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-i-the-fundamentals/lecture-overview-9
title: Lecture Overview
transcript: >-
  <p><span m='230'>In this lecture we complete our discussion of multiple</span>
  <span m='3560'>continuous random variables.</span> </p><p><span m='6350'>In
  the first half, we talk about the conditional</span> <span
  m='8850'>distribution of one random variable, given</span> <span m='11160'>the
  value of another.</span> </p><p><span m='13160'>We will see that the mechanics
  are essentially the same as in</span> <span m='16120'>the discrete
  case.</span> </p><p><span m='18180'>Here, we will actually face some subtle
  issues, because we</span> <span m='21950'>will be conditioning on any event
  that has 0 probability.</span> </p><p><span m='26040'>Nevertheless, all
  formulas will still have the form that</span> <span m='29590'>one should
  expect.</span> </p><p><span m='30960'>And in particular, we will see natural
  versions of the total</span> <span m='34220'>probability and total expectation
  theorems.</span> </p><p><span m='38580'>We will also define independence of
  continuous</span> <span m='41510'>random variables, a concept that has the
  same intuitive</span> <span m='44850'>content as in the discrete case.</span>
  </p><p><span m='47290'>That is, when we have independent random
  variables,</span> <span m='50410'>the values of some of them do not cause any
  revision of our</span> <span m='54420'>beliefs about the remaining
  ones.</span> </p><p><span m='57590'>Then, in the second half of the lecture,
  we will focus on</span> <span m='61090'>the Bayes rule.</span> </p><p><span
  m='62550'>This will be the methodological foundation for</span> <span
  m='65069'>when, later in this course, we dive into</span> <span m='68120'>the
  subject of inference.</span> </p><p><span m='70490'>The Bayes rule allows us
  to revise our beliefs about a</span> <span m='73660'>random variable.</span>
  </p><p><span m='74670'>That is, replace an original probability distribution
  by a</span> <span m='78370'>conditional one, after we observe the value of
  some</span> <span m='81920'>other random variable.</span> </p><p><span
  m='84770'>Depending on whether the random variables involved are</span> <span
  m='87539'>discrete or continuous, we will get four different</span> <span
  m='91660'>versions of the Bayes rule.</span> </p><p><span m='93890'>They all
  have the same form, with small differences.</span> </p><p><span m='97970'>And
  we will see how to apply them through some examples.</span> </p><p></p>
uid: 4ceeb1fd8ebd21c8c473cf3c336324f5
type: courses
layout: video
---
