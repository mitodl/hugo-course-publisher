---
about_this_resource_text: '<p><strong>Instructor:</strong> John Tsitsiklis</p>'
embedded_media:
  - id: Video-YouTube-Stream
    media_location: TbRh71BMJvw
    parent_uid: 1bb7e459758e7ff8608e14883de0bb45
    title: Video-YouTube-Stream
    type: Video
    uid: a6cece869da2b52e5145bda272ef47a1
  - id: Thumbnail-YouTube-JPG
    media_location: 'https://img.youtube.com/vi/TbRh71BMJvw/default.jpg'
    parent_uid: 1bb7e459758e7ff8608e14883de0bb45
    title: Thumbnail-YouTube-JPG
    type: Thumbnail
    uid: e3896e94a45f2181c58454fccd40b30d
  - id: 3Play-3PlayYouTubeid-MP4
    media_location: TbRh71BMJvw
    parent_uid: 1bb7e459758e7ff8608e14883de0bb45
    title: 3Play-3Play YouTube id
    type: 3Play
    uid: 38717d807df7f7efbbd1222e55801c18
  - id: TbRh71BMJvw.srt
    parent_uid: 1bb7e459758e7ff8608e14883de0bb45
    technical_location: >-
      https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-i-the-fundamentals/linearity-of-expectations-the-mean-of-the-binomial/TbRh71BMJvw.srt
    title: 3play caption file
    type: null
    uid: c4c99ec8cbbc6d58cdf74e5df0f64801
  - id: TbRh71BMJvw.pdf
    parent_uid: 1bb7e459758e7ff8608e14883de0bb45
    technical_location: >-
      https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-i-the-fundamentals/linearity-of-expectations-the-mean-of-the-binomial/TbRh71BMJvw.pdf
    title: 3play pdf file
    type: null
    uid: f395f07c1c1ecd24f01e6bcc0f8b4114
  - id: Caption-3Play YouTube id-SRT
    parent_uid: 1bb7e459758e7ff8608e14883de0bb45
    title: Caption-3Play YouTube id-SRT-English - US
    type: Caption
    uid: ffc309add257a4f0484d7f7e86a973c1
  - id: Transcript-3Play YouTube id-PDF
    parent_uid: 1bb7e459758e7ff8608e14883de0bb45
    title: Transcript-3Play YouTube id-PDF-English - US
    type: Transcript
    uid: 1973bc8863991b86b871f02ca7b6742d
  - id: Video-InternetArchive-MP4
    media_location: >-
      https://archive.org/download/MITRES.6-012S18/MITRES6_012S18_L06-08_300k.mp4
    parent_uid: 1bb7e459758e7ff8608e14883de0bb45
    title: Video-Internet Archive-MP4
    type: Video
    uid: 25207ff27f57fde9f311c327752322f4
inline_embed_id: 60185552linearityofexpectationsthemeanofthebinomial18385812
order_index: 636
parent_uid: 9ca6b310dc93095c9ac0f0e5f95e6930
related_resources_text: ''
short_url: linearity-of-expectations-the-mean-of-the-binomial
technical_location: >-
  https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-i-the-fundamentals/linearity-of-expectations-the-mean-of-the-binomial
title: Linearity of Expectations & the Mean of the Binomial
transcript: >-
  <p><span m="1510">Let us now revisit the subject of expectations and develop
  an</span> <span m="5570">important linearity property for the case where
  we're</span> <span m="8580">dealing with multiple random
  variables.</span></p><p><span m="11330">We already have a linearity
  property.</span></p><p><span m="14420">If we have a linear function of a
  single random variable,</span> <span m="18350">then expectations behave in a
  linear fashion.</span></p><p><span m="22220">But now, if we have multiple
  random variables, we have this</span> <span m="25370">additional
  property.</span></p><p><span m="26930">The expected value of the sum of two
  random variables is</span> <span m="30630">equal to the sum of their
  expectations.</span></p><p><span m="34340">Let us go through the derivation of
  this very</span> <span m="37610">important fact because it is a nice exercise
  in applying the</span> <span m="42030">expected value rule and also
  manipulating</span> <span m="45630">PMFs and joint PMFs.</span></p><p><span
  m="49960">We're dealing with the expected value of a function</span> <span
  m="54740">of two random variables.</span></p><p><span m="58110">Which
  function?</span></p><p><span m="59700">If we write it this way, we are dealing
  with the function</span> <span m="68300">g, which is just the sum of its two
  entries.</span></p><p><span m="77230">So now we can continue with the
  application of the</span> <span m="80760">expected value
  rule.</span></p><p><span m="82710">And we obtain the sum over all possible x,
  y pairs.</span></p><p><span m="88039">Here, we need to write to g of
  x,y.</span></p><p><span m="90750">But in our case, the function we're dealing
  with</span> <span m="93759">is just x plus y.</span></p><p><span m="96789">And
  then we weigh, according to the entries</span> <span m="100210">of the joint
  PMF.</span></p><p><span m="101580">So this is just an application of the
  expected value rule to</span> <span m="105289">this particular
  function.</span></p><p><span m="107729">Now let us take this sum and break it
  into two pieces--</span> <span m="112740">one involving only the x-term, and
  another piece involving</span> <span m="122870">only the
  y-term.</span></p><p><span m="134640">Now, if we look at this double
  summation, look</span> <span m="140850">at the inner sum.</span></p><p><span
  m="142540">It's a sum over y's.</span></p><p><span m="144490">While we're
  adding over y's, the value of x remains fixed.</span></p><p><span
  m="148420">So x is a constant, as far as the sum is
  concerned.</span></p><p><span m="151900">So x can be pulled outside this
  summation.</span></p><p><span m="168160">Let us just continue with this term,
  the first one, and see</span> <span m="174930">that a simplification
  happens.</span></p><p><span m="177570">This quantity here is the sum of the
  probabilities of the</span> <span m="181290">different y's that can go
  together with a particular x.</span></p><p><span m="185150">So it is just
  equal to the probability or</span> <span m="187740">that particular
  x.</span></p><p><span m="188940">It's the marginal PMF.</span></p><p><span
  m="197630">If we carry out a similar step for the second term, we will</span>
  <span m="201600">obtain the sum over y's.</span></p><p><span m="203780">It's
  just a symmetrical argument.</span></p><p><span m="207940">And at this point
  we recognize that what we have in front of</span> <span m="211579">us is just
  the expected value of X, this is the first term,</span> <span m="216520">plus
  the expected value of Y. So this completes the</span> <span
  m="220170">derivation of this important linearity property.</span></p><p><span
  m="225500">Of course, we proved the linearity property for the</span> <span
  m="227900">case of the sum of two random variables.</span></p><p><span
  m="231460">But you can proceed in a similar way, or maybe use</span> <span
  m="235130">induction, and one can easily establish, by following the</span>
  <span m="239930">same kind of argument, that we have a linearity property
  when</span> <span m="243100">we add any finite number of random
  variables.</span></p><p><span m="247320">The expected value of a sum is the
  sum of</span> <span m="249750">the expected values.</span></p><p><span
  m="252720">Just for a little bit of practice, if, for example,</span> <span
  m="256450">we're dealing with this expression, the expected value</span> <span
  m="259200">of that expression would be the expected value of 2X plus</span>
  <span m="265510">the expected value of 3Y minus the expected value of Z.
  And</span> <span m="273370">then, using the linearity property for linear
  functions</span> <span m="277440">of a single random variable, we can pull the
  constants out</span> <span m="281380">of the expectations.</span></p><p><span
  m="282510">And this would be twice the expected value of X plus 3</span> <span
  m="286330">times the expected value of Y minus the expected value of
  Z.</span></p><p><span m="296380">What we will do next is to use the linearity
  property of</span> <span m="300160">expectations to solve a problem that would
  otherwise</span> <span m="304920">be quite difficult.</span></p><p><span
  m="307540">We will use the linearity property to find the mean of a</span>
  <span m="312040">binomial random variable.</span></p><p><span m="314350">Let X
  be a binomial random variable with</span> <span m="317170">parameters n and
  p.</span></p><p><span m="318830">And we can interpret X as the number of
  successes in n</span> <span m="322780">independent trials where each one of
  the trials has a</span> <span m="325880">probability p of resulting in a
  success.</span></p><p><span m="329320">Well, we know the PMF of a
  binomial.</span></p><p><span m="332470">And we can use the definition of
  expectation to obtain this</span> <span
  m="337420">expression.</span></p><p><span m="338690">This is just the PMF of
  the binomial.</span></p><p><span m="346040">And therefore, what we have here
  is the usual definition</span> <span m="349240">of the expected
  value.</span></p><p><span m="350800">Now, if you look at this sum, it appears
  quite formidable.</span></p><p><span m="354750">And it would be quite hard to
  evaluate it.</span></p><p><span m="358390">Instead, we're going to use a very
  useful trick.</span></p><p><span m="362190">We will employ what we have called
  indicator variables.</span></p><p><span m="367950">So let's define a random
  variable Xi, which is a one if</span> <span m="371880">the ith trial is a
  success, and zero otherwise.</span></p><p><span m="376050">Now if we want to
  count successes, what we want to</span> <span m="380050">count is how many of
  the Xi's are equal to 1.</span></p><p><span m="384850">So if we add the Xi's,
  this will have a contribution of 1</span> <span m="390480">from each one of
  the successes.</span></p><p><span m="392320">So when you add them up, you
  obtain the</span> <span m="394490">total number of
  successes.</span></p><p><span m="396710">So we have expressed a random
  variable as a sum of much</span> <span m="400659">simpler random
  variables.</span></p><p><span m="403270">So at this point, we can now use
  linearity of expectations</span> <span m="407620">to write that the expected
  value of X will be the</span> <span m="411400">expected value of X1 plus all
  the way to the</span> <span m="417330">expected value of
  Xn.</span></p><p><span m="420920">Now what is the expected value of
  X1?</span></p><p><span m="425180">It is a Bernoulli random variable that takes
  the value</span> <span m="428920">1 with probability p and takes the value of
  0 with</span> <span m="433060">probability 1 minus p.</span></p><p><span
  m="434940">The expected value of this random variable is p.</span></p><p><span
  m="439370">And similarly, for each one of these terms in the
  summation.</span></p><p><span m="443830">And so the final end result is equal
  to n times p.</span></p><p><span m="449310">This answer, of course, makes also
  intuitive sense.</span></p><p><span m="452770">If we have to p equal to 1/2,
  and we toss a coin 100 times,</span> <span m="460190">the expected number, or
  the average number, of heads we</span> <span m="465230">expect to see will be
  1/2 half times 100, which is 50.</span></p><p><span m="470650">The higher p
  is, the more successes we expect to see.</span></p><p><span m="475270">And of
  course, if we double n, we expect to see</span> <span m="478480">twice as many
  successes.</span></p><p><span m="481120">So this is an illustration of the
  power of breaking up</span> <span m="485760">problems into simpler pieces that
  are easier to analyze.</span></p><p><span m="490150">And the linearity of
  expectations is one more tool</span> <span m="494440">that we have in our
  hands for breaking up perhaps</span> <span m="498710">complicated random
  variables into simpler ones and then</span> <span m="502220">analyzing them
  separately.</span></p><p>&nbsp;</p>
uid: 1bb7e459758e7ff8608e14883de0bb45
type: course
layout: video
---
