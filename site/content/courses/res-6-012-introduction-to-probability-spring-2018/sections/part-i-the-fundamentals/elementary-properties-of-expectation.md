---
about_this_resource_text: '<p><strong>Instructor:</strong> John Tsitsiklis</p>'
embedded_media:
  - id: Video-YouTube-Stream
    media_location: GARQ31BrKQA
    parent_uid: 128dd7e087c7e91ae5c7dad936b59367
    title: Video-YouTube-Stream
    type: Video
    uid: bbb26ba6eefd2a39b338f7de613b4975
  - id: Thumbnail-YouTube-JPG
    media_location: 'https://img.youtube.com/vi/GARQ31BrKQA/default.jpg'
    parent_uid: 128dd7e087c7e91ae5c7dad936b59367
    title: Thumbnail-YouTube-JPG
    type: Thumbnail
    uid: e15c4d7138ed1c854a46712f1e58119c
  - id: 3Play-3PlayYouTubeid-MP4
    media_location: GARQ31BrKQA
    parent_uid: 128dd7e087c7e91ae5c7dad936b59367
    title: 3Play-3Play YouTube id
    type: 3Play
    uid: 82bda2b7ca28a6495b901bfc7770fed7
  - id: GARQ31BrKQA.srt
    parent_uid: 128dd7e087c7e91ae5c7dad936b59367
    technical_location: >-
      https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-i-the-fundamentals/elementary-properties-of-expectation/GARQ31BrKQA.srt
    title: 3play caption file
    type: null
    uid: ed1cfede9eb66fe632d8fc4996d9b2e5
  - id: GARQ31BrKQA.pdf
    parent_uid: 128dd7e087c7e91ae5c7dad936b59367
    technical_location: >-
      https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-i-the-fundamentals/elementary-properties-of-expectation/GARQ31BrKQA.pdf
    title: 3play pdf file
    type: null
    uid: 7d55b24175c3cd158d47251cdf23ee30
  - id: Caption-3Play YouTube id-SRT
    parent_uid: 128dd7e087c7e91ae5c7dad936b59367
    title: Caption-3Play YouTube id-SRT-English - US
    type: Caption
    uid: f5fd7c09a73eb4a3a9f3c171cb9f3752
  - id: Transcript-3Play YouTube id-PDF
    parent_uid: 128dd7e087c7e91ae5c7dad936b59367
    title: Transcript-3Play YouTube id-PDF-English - US
    type: Transcript
    uid: bc752035ba9c98c40c4621a5577bcf76
  - id: Video-InternetArchive-MP4
    media_location: >-
      https://archive.org/download/MITRES.6-012S18/MITRES6_012S18_L05-09_300k.mp4
    parent_uid: 128dd7e087c7e91ae5c7dad936b59367
    title: Video-Internet Archive-MP4
    type: Video
    uid: 22ca9e6757b0251b2e79a3bed7bcef9e
inline_embed_id: 22920543elementarypropertiesofexpectation70309848
order_index: 537
parent_uid: 9ca6b310dc93095c9ac0f0e5f95e6930
related_resources_text: ''
short_url: elementary-properties-of-expectation
technical_location: >-
  https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-i-the-fundamentals/elementary-properties-of-expectation
title: Elementary Properties of Expectation
transcript: >-
  <p><span m="70">We now note some elementary properties of
  expectations.</span></p><p><span m="3670">These will be some properties that
  are extremely natural and</span> <span m="6710">intuitive, but even so, they
  are worth recording.</span></p><p><span m="11650">The first property is the
  following.</span></p><p><span m="13050">If you have a random variable which is
  non-negative, then</span> <span m="17170">its expected value is also
  non-negative.</span></p><p><span m="19950">What does it mean that the random
  variable is</span> <span m="21830">non-negative?</span></p><p><span
  m="23220">What it means is that for all possible outcomes of the</span> <span
  m="27340">experiment, no matter what the outcome is, the associated</span>
  <span m="31680">numerical value of the random variable is a</span> <span
  m="34540">non-negative number.</span></p><p><span m="36570">What's the
  implication of this?</span></p><p><span m="39080">When we calculate an
  expectation we're adding over</span> <span m="42070">all the possible
  numerical values of the random variable.</span></p><p><span m="45630">All the
  possible numerical values of the random variable</span> <span m="49270">under
  this assumption are non-negative.</span></p><p><span m="52320">Probabilities
  are also non-negative.</span></p><p><span m="55650">So we have a sum of
  non-negative entries and</span> <span m="59440">therefore, the expected value
  is also going to be</span> <span m="62580">non-negative.</span></p><p><span
  m="65230">The next property is a generalization of this.</span></p><p><span
  m="69870">Consider now a random variable that has the property that no</span>
  <span m="74260">matter what the outcome of the experiment is, the value
  of</span> <span m="79440">this random variable lies in the range between
  two</span> <span m="83090">constants, a and b.</span></p><p><span m="85330">In
  this case, we argue as follows.</span></p><p><span m="87580">The expected
  value, by definition, is a sum over all</span> <span m="92000">possible values
  of the random variable of certain terms.</span></p><p><span m="96750">Now, the
  possible numerical values of the random variable</span> <span m="100570">are
  all of them at least as large as a, so this gives us</span> <span
  m="106190">an inequality of this type.</span></p><p><span m="109780">Then, we
  pull a factor of a outside of the summation.</span></p><p><span m="118740">And
  finally, we recall that the sum of a PMF over all</span> <span
  m="125660">possible values of little x is equal to 1.</span></p><p><span
  m="129759">Why is that the case?</span></p><p><span m="131290">Well, these are
  the probabilities for the</span> <span m="133600">different numerical values
  of the random variable.</span></p><p><span m="136360">The sum of the
  probabilities of all the possible numerical</span> <span m="139650">values has
  to be equal to 1, because that exhausts all the</span> <span
  m="143260">possibilities.</span></p><p><span m="144440">So we obtain a times
  1, which is a.</span></p><p><span m="148186">So, what we have proved is that
  the expected value is at</span> <span m="152079">least large as
  a.</span></p><p><span m="154210">You can use a symmetrical argument where
  the</span> <span m="157110">inequalities will go the opposite way and where
  a's</span> <span m="160200">will be replaced by b's, to prove the
  second</span> <span m="163540">inequality, as well.</span></p><p><span
  m="166870">The last fact we want to take note of is the
  following.</span></p><p><span m="170450">If we have a constant and we take its
  expected value, we</span> <span m="173600">obtain the same
  constant.</span></p><p><span m="175530">What does that
  mean?</span></p><p><span m="176980">We have only been talking about expected
  values of</span> <span m="179540">random variables.</span></p><p><span
  m="180440">What does it mean to take the expected value of a
  constant?</span></p><p><span m="183750">Well, as we discussed earlier, we can
  think of a constant as</span> <span m="188100">being a random variable of a
  very special type.</span></p><p><span m="192560">A random variable whose PMF
  takes this form.</span></p><p><span m="196690">This random variable can take
  only a single value and the</span> <span m="200280">probability of that single
  value is equal to 1.</span></p><p><span m="204230">This means that in the
  formula for the expected value there's</span> <span m="207540">going to be
  only one term in this summation, and that term</span> <span m="213120">is
  going to be c times the probability that our random</span> <span
  m="220190">variable takes the value c.</span></p><p><span m="224820">Now, that
  probability is equal to 1, and we're left with c.</span></p><p><span
  m="228280">So this equality makes sense, of course, as long as you</span>
  <span m="228335">understand that a constant can also be viewed as a
  random</span> <span m="228390">variable of a very degenerate
  type.</span></p><p><span m="228420">Now, intuitively, of course, it's
  certainly clear</span> <span m="228454">what this is
  saying.</span></p><p><span m="228475">That if a certain quantity is always
  equal to c, then on the</span> <span m="228540">average, it will also be equal
  to c.</span></p><p>&nbsp;</p>
uid: 128dd7e087c7e91ae5c7dad936b59367
type: courses
layout: video
---
