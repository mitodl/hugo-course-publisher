---
about_this_resource_text: '<p><strong>Instructor:</strong> John Tsitsiklis</p>'
embedded_media:
  - id: Video-YouTube-Stream
    media_location: 0cD-tcITuck
    parent_uid: 7dc21bf782eaff4a72aed1e7693a1aff
    title: Video-YouTube-Stream
    type: Video
    uid: d8c9f00592d4d506568c7efdafce682a
  - id: Thumbnail-YouTube-JPG
    media_location: 'https://img.youtube.com/vi/0cD-tcITuck/default.jpg'
    parent_uid: 7dc21bf782eaff4a72aed1e7693a1aff
    title: Thumbnail-YouTube-JPG
    type: Thumbnail
    uid: 7df0f716fccef2dc5d34fca7015bba58
  - id: 3Play-3PlayYouTubeid-MP4
    media_location: 0cD-tcITuck
    parent_uid: 7dc21bf782eaff4a72aed1e7693a1aff
    title: 3Play-3Play YouTube id
    type: 3Play
    uid: 7308d3eea6430f6e977913ceb2de9460
  - id: 0cD-tcITuck.srt
    parent_uid: 7dc21bf782eaff4a72aed1e7693a1aff
    technical_location: >-
      https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-i-the-fundamentals/total-probability-total-expectation-theorems/0cD-tcITuck.srt
    title: 3play caption file
    type: null
    uid: 357bdbfbf43288da4aba9aaa9700d283
  - id: 0cD-tcITuck.pdf
    parent_uid: 7dc21bf782eaff4a72aed1e7693a1aff
    technical_location: >-
      https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-i-the-fundamentals/total-probability-total-expectation-theorems/0cD-tcITuck.pdf
    title: 3play pdf file
    type: null
    uid: db8b2363bd5a93f7f81a70939e84a57a
  - id: Caption-3Play YouTube id-SRT
    parent_uid: 7dc21bf782eaff4a72aed1e7693a1aff
    title: Caption-3Play YouTube id-SRT-English - US
    type: Caption
    uid: 77005b5c12d20823a7ea92fb7a65017d
  - id: Transcript-3Play YouTube id-PDF
    parent_uid: 7dc21bf782eaff4a72aed1e7693a1aff
    title: Transcript-3Play YouTube id-PDF-English - US
    type: Transcript
    uid: 87c549be17b6acba9f197605aa04ea60
  - id: Video-InternetArchive-MP4
    media_location: >-
      https://archive.org/download/MITRES.6-012S18/MITRES6_012S18_L10-04_300k.mp4
    parent_uid: 7dc21bf782eaff4a72aed1e7693a1aff
    title: Video-Internet Archive-MP4
    type: Video
    uid: abc7c199368c9268acb31cbe31ac8d8d
inline_embed_id: 80339384totalprobabilitytotalexpectationtheorems53365560
order_index: 942
parent_uid: 9ca6b310dc93095c9ac0f0e5f95e6930
related_resources_text: ''
short_url: total-probability-total-expectation-theorems
technical_location: >-
  https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-i-the-fundamentals/total-probability-total-expectation-theorems
title: Total Probability & Total Expectation Theorems
transcript: >-
  <p><span m='1870'>Conditional PDFs share most of the properties</span> <span
  m='4620'>of conditional PMFs.</span> </p><p><span m='6370'>All facts for the
  discrete case have continuous analogs.</span> </p><p><span m='9950'>The
  intuition is more or less the same, although it is much</span> <span
  m='13010'>easier to grasp in the discrete case.</span> </p><p><span
  m='15760'>For example, we have seen this version of the total</span> <span
  m='18390'>probability theorem.</span> </p><p><span m='20260'>There is a
  continuous analog in which we</span> <span m='22710'>replace sums by
  integrals.</span> </p><p><span m='25660'>And we replace PMFs by PDFs.</span>
  </p><p><span m='29100'>The proof of this fact is actually pretty
  simple.</span> </p><p><span m='32380'>By the multiplication rule, the
  integrand, here, is just</span> <span m='36820'>the joint PDF of X and
  Y.</span> </p><p><span m='41350'>And we know that if we take the joint PDF and
  integrate</span> <span m='45370'>with respect to one variable then we recover
  the marginal</span> <span m='50880'>PDF of the other random variable.</span>
  </p><p><span m='54470'>So this is one theorem that extends to</span> <span
  m='57700'>the continuous case.</span> </p><p><span m='60150'>Moving along, we
  have defined the conditional expectation in</span> <span m='63470'>this manner
  in the discrete case.</span> </p><p><span m='65810'>And we define it similarly
  for the continuous case.</span> </p><p><span m='69610'>So actually here we now
  have a new definition.</span> </p><p><span m='73660'>This definition is also
  consistent with the definition</span> <span m='76710'>of the expectation of a
  continuous random variable.</span> </p><p><span m='80230'>The expected value
  for continuous random variable is</span> <span m='82720'>the integral of X
  times [a]</span> <span m='84890'>density.</span> </p><p><span m='85830'>Except
  that here we live in a conditional universe where</span> <span m='88440'>we're
  conditioning on this event.</span> </p><p><span m='90770'>And therefore, we
  need to use the</span> <span m='92479'>corresponding conditional PDF.</span>
  </p><p><span m='95670'>Finally, we have the total expectation theorem in
  the</span> <span m='100080'>discrete case.</span> </p><p><span m='101150'>And
  there is the obvious analog in the continuous case</span> <span
  m='104289'>where we are using an integral and a density.</span> </p><p><span
  m='108479'>The interpretation is that we consider all possibilities for</span>
  <span m='112490'>Y. Under each possibility of Y we find the expected value
  of</span> <span m='116960'>X. And then we weigh those different
  possibilities</span> <span m='120850'>according to the corresponding values of
  the density.</span> </p><p><span m='123790'>So we're taking a weighted average
  of these conditional</span> <span m='126580'>expectations to obtain the
  overall expectation of the</span> <span m='129600'>random variable X.</span>
  </p><p><span m='132190'>The derivation of this fact is maybe a little
  instructive</span> <span m='137640'>because it uses various facts that we have
  in our hands.</span> </p><p><span m='141900'>So let's see how to derive
  it.</span> </p><p><span m='144060'>We start from this expression in the
  right-hand side and we</span> <span m='147010'>will show that it is equal to
  the expected value of X. The</span> <span m='150740'>expression on the
  right-hand side is equal to the</span> <span m='153910'>following, it's the
  integral of the density of Y.</span> </p><p><span m='158910'>And then, inside
  here, we have the conditional expectation</span> <span m='162030'>which is
  defined this way.</span> </p><p><span m='163700'>So we just plug-in the
  definition.</span> </p><p><span m='174590'>And then what we do, is we take
  this term and move it</span> <span m='179380'>inside the integral.</span>
  </p><p><span m='184030'>Which we can do because this integral is with respect
  to x.</span> </p><p><span m='187570'>And therefore, this is like a
  constant.</span> </p><p><span m='201750'>And we also interchange the order of
  integration.</span> </p><p><span m='207190'>Now, the inner integration is with
  respect to y.</span> </p><p><span m='211300'>As far as Y is concerned, this
  term, x, is a constant.</span> </p><p><span m='216650'>So we can take it and
  move it outside this first integral</span> <span m='223020'>and place it out
  there.</span> </p><p><span m='225350'>So this term disappears and goes out
  there.</span> </p><p><span m='229390'>What do we have here?</span>
  </p><p><span m='232850'>This part, by the previous fact, the total
  probability</span> <span m='238940'>theorem, is just the density of X. So
  we're left with the</span> <span m='243600'>integral of x times the density of
  x dx.</span> </p><p><span m='251560'>And this is the expected value of
  X.</span> </p><p><span m='256959'>Finally, we have various forms of the
  expected value rule,</span> <span m='260600'>which barely deserve writing
  down.</span> </p><p><span m='262380'>Because they're exactly what you might
  expect.</span> </p><p><span m='265460'>Consider, for example, an expression
  such as this one,</span> <span m='269450'>the expected value of a function of
  the random</span> <span m='272330'>variable X but conditioned on the value of
  the random</span> <span m='278270'>variable Y. How do we calculate this
  quantity?</span> </p><p><span m='281909'>Well, the expected value rule tells
  us that we should</span> <span m='284840'>integrate g of x times the density
  of X. But because,</span> <span m='294110'>here we live in a conditional
  universe, we should actually</span> <span m='297740'>use the corresponding
  conditional PDF of X. And</span> <span m='302750'>there are many other
  versions of the expected value rule.</span> </p><p><span m='305780'>Any
  version that we have seen for the discrete case has,</span> <span
  m='309010'>also, a continuous analog which looks about the same</span> <span
  m='312530'>except that we integrate and that we use densities.</span>
  </p><p></p>
uid: 7dc21bf782eaff4a72aed1e7693a1aff
type: courses
layout: video
---
