---
about_this_resource_text: '<p><strong>Instructor:</strong> John Tsitsiklis</p>'
embedded_media:
  - id: Video-YouTube-Stream
    media_location: GH7dwoXSD0s
    parent_uid: 09d98ba3f6dd40acd69004ec20dde9fd
    title: Video-YouTube-Stream
    type: Video
    uid: 7f6c89c90d600dbe6e8f38cd5f930e10
  - id: Thumbnail-YouTube-JPG
    media_location: 'https://img.youtube.com/vi/GH7dwoXSD0s/default.jpg'
    parent_uid: 09d98ba3f6dd40acd69004ec20dde9fd
    title: Thumbnail-YouTube-JPG
    type: Thumbnail
    uid: 8da9c1d3ec5f9520eec9e20e5d37e399
  - id: 3Play-3PlayYouTubeid-MP4
    media_location: GH7dwoXSD0s
    parent_uid: 09d98ba3f6dd40acd69004ec20dde9fd
    title: 3Play-3Play YouTube id
    type: 3Play
    uid: 5e7212ad460f50bcf4268b17c3cf6536
  - id: GH7dwoXSD0s.srt
    parent_uid: 09d98ba3f6dd40acd69004ec20dde9fd
    technical_location: >-
      https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-i-the-fundamentals/the-variance-of-the-sum-of-random-variables/GH7dwoXSD0s.srt
    title: 3play caption file
    type: null
    uid: c4c6e54ea53230466f4a5cf110dea433
  - id: GH7dwoXSD0s.pdf
    parent_uid: 09d98ba3f6dd40acd69004ec20dde9fd
    technical_location: >-
      https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-i-the-fundamentals/the-variance-of-the-sum-of-random-variables/GH7dwoXSD0s.pdf
    title: 3play pdf file
    type: null
    uid: 4db7bc1d0e94ba77338301aa22f08900
  - id: Caption-3Play YouTube id-SRT
    parent_uid: 09d98ba3f6dd40acd69004ec20dde9fd
    title: Caption-3Play YouTube id-SRT-English - US
    type: Caption
    uid: cdf760c189407e9c403686b48c840399
  - id: Transcript-3Play YouTube id-PDF
    parent_uid: 09d98ba3f6dd40acd69004ec20dde9fd
    title: Transcript-3Play YouTube id-PDF-English - US
    type: Transcript
    uid: 00387d56ee608cc1d018dd9d67d88b31
  - id: Video-InternetArchive-MP4
    media_location: >-
      https://archive.org/download/MITRES.6-012S18/MITRES6_012S18_L12-07_300k.mp4
    parent_uid: 09d98ba3f6dd40acd69004ec20dde9fd
    title: Video-Internet Archive-MP4
    type: Video
    uid: 2768ad62b5603bef037023b98e2e9462
inline_embed_id: 42308914thevarianceofthesumofrandomvariables91139987
order_index: 1149
parent_uid: 9ca6b310dc93095c9ac0f0e5f95e6930
related_resources_text: ''
short_url: the-variance-of-the-sum-of-random-variables
technical_location: >-
  https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-i-the-fundamentals/the-variance-of-the-sum-of-random-variables
title: The Variance of the Sum of Random Variables
transcript: >-
  <p><span m='880'>One situation where covariances show up</span> <span
  m='3730'>is when we try to calculate the variance</span> <span m='6110'>of a
  sum of random variables.</span> </p><p><span m='8360'>So let us look at the
  variance of the sum</span> <span m='10210'>of two random variables, X1 and
  X2.</span> </p><p><span m='13120'>If the two random variables are
  independent,</span> <span m='15520'>then we know that the variance of the
  sum</span> <span m='17650'>is the sum of the variances.</span> </p><p><span
  m='20040'>Let us now look at what happens in the case where</span> <span
  m='23290'>we may have dependence.</span> </p><p><span m='26120'>By definition,
  the variance is the expected value</span> <span m='30350'>of the difference of
  the random variable we're</span> <span m='32900'>interested in from its
  expected value, squared.</span> </p><p><span m='43480'>And now we rearrange
  terms here and write</span> <span m='47130'>what is inside the expectation as
  follows.</span> </p><p><span m='51180'>We put together X1 with the term minus
  the expected value of X1</span> <span m='60640'>and then X2 together with
  negative the expected value</span> <span m='67170'>of X2.</span> </p><p><span
  m='75590'>So now we have the square of the sum of two terms.</span>
  </p><p><span m='81270'>We expand the quadratic to obtain</span> <span
  m='85000'>expected value of the square of the first term</span> <span
  m='92250'>plus the square of the second term plus 2 times a cross term.</span>
  </p><p><span m='116770'>And what do we have here?</span> </p><p><span
  m='118890'>The expected value of the first term</span> <span m='121210'>is
  just the variance of X1.</span> </p><p><span m='124970'>The expected value of
  this second term</span> <span m='128419'>is just the variance of X2.</span>
  </p><p><span m='132740'>And finally, the cross term, the expected value of
  it,</span> <span m='137079'>we recognize that it is the same as the covariance
  of X1</span> <span m='141370'>with X2.</span> </p><p><span m='142590'>And we
  also have this factor of 2 up here.</span> </p><p><span m='146520'>So this is
  the general form for the variance</span> <span m='149260'>of the sum of two
  random variables.</span> </p><p><span m='152040'>In the case of independence,
  the covariance is 0,</span> <span m='155730'>and we just have the sum of the
  two variances.</span> </p><p><span m='158520'>But when the random variables
  are dependent,</span> <span m='161300'>it is possible that the covariance will
  be non-zero,</span> <span m='164240'>and we have one additional term.</span>
  </p><p><span m='166940'>Let us now not generalize this calculation.</span>
  </p><p><span m='169860'>Here is for reference and comparison</span> <span
  m='171970'>the formula for the case where we add two random variables.</span>
  </p><p><span m='175430'>But now let us look at the variance</span> <span
  m='177140'>of the sum of many of them.</span> </p><p><span m='179220'>To keep
  the calculation simple, we're</span> <span m='181380'>going to assume that the
  means are zero.</span> </p><p><span m='185230'>But the final conclusion will
  also</span> <span m='187390'>be valid for the case of non-zero means.</span>
  </p><p><span m='190460'>Since we have assumed zero means,</span> <span
  m='192810'>the variance is the same as the expected value</span> <span
  m='195740'>of the square of the random variable involved,</span> <span
  m='198890'>which is this one.</span> </p><p><span m='201130'>And now we expand
  this quadratic to obtain the expected value</span> <span m='207190'>of: we
  will have a bunch of terms</span> <span m='210620'>of this, where i ranges
  from 1 up to n.</span> </p><p><span m='216800'>And then we will have a bunch
  of cross terms of the form Xi, Xj.</span> </p><p><span m='223180'>And we
  obtain one cross term for each choice of i</span> <span m='228380'>from 1 to n
  and for each choice of j from 1 to n,</span> <span m='235020'>as long as i is
  different from j.</span> </p><p><span m='238566'>So overall here, this sum
  will have n squared minus n terms.</span> </p><p><span m='248810'>Now, we use
  linearity to move the expectation</span> <span m='253840'>inside the
  summation.</span> </p><p><span m='255800'>And so from here, we obtain the
  sum</span> <span m='261240'>of the expected value of Xi squared, which</span>
  <span m='265940'>is the same as the variance of Xi,</span> <span
  m='268820'>since we assumed zero means.</span> </p><p><span m='271630'>And
  similarly here, we're going to get this double sum over i's</span> <span
  m='276700'>that are different from j of the expected value of Xi, Xj.</span>
  </p><p><span m='281460'>And in the case of 0 means again, this</span> <span
  m='284270'>is the same as the covariance of Xi with Xj.</span> </p><p><span
  m='288830'>And so we have obtained this general formula</span> <span
  m='292490'>that gives us the variance of a sum of random variables.</span>
  </p><p><span m='296470'>If the random variables have 0 covariances,</span>
  <span m='299220'>then the variance of the sum is the sum of the
  variances.</span> </p><p><span m='302740'>And this happens in particular when
  the random variables</span> <span m='305460'>are independent.</span>
  </p><p><span m='306850'>For the general case, where we may have
  dependencies</span> <span m='309940'>and non-zero variances, then the variance
  of the sum</span> <span m='313310'>involves also all the possible
  covariances</span> <span m='317320'>between the different random
  variables.</span> </p><p><span m='320220'>And let me finally add that this
  formula is also</span> <span m='323540'>valid for the general case where we do
  not</span> <span m='326380'>assume that the means are zero.</span>
  </p><p><span m='328690'>And the derivation is very similar,</span> <span
  m='331280'>except that there's a few more symbols</span> <span m='333300'>that
  are floating around.</span> </p><p></p>
uid: 09d98ba3f6dd40acd69004ec20dde9fd
type: course
layout: video
---
