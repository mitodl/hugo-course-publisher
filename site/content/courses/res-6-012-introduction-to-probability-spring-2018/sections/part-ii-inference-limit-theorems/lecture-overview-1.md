---
about_this_resource_text: '<p><strong>Instructor:</strong> John Tsitsiklis</p>'
embedded_media:
  - id: Video-YouTube-Stream
    media_location: FMrYw7sgyxQ
    parent_uid: d7daadd47b972104cb7537f96c5d8c0f
    title: Video-YouTube-Stream
    type: Video
    uid: 679eb8111cd96dc354c1eb08c18fe9c0
  - id: Thumbnail-YouTube-JPG
    media_location: 'https://img.youtube.com/vi/FMrYw7sgyxQ/default.jpg'
    parent_uid: d7daadd47b972104cb7537f96c5d8c0f
    title: Thumbnail-YouTube-JPG
    type: Thumbnail
    uid: 8c8f93f75c8d406dd677b0ea1cd606cb
  - id: 3Play-3PlayYouTubeid-MP4
    media_location: FMrYw7sgyxQ
    parent_uid: d7daadd47b972104cb7537f96c5d8c0f
    title: 3Play-3Play YouTube id
    type: 3Play
    uid: ff62e7c7bffa4d027e465fb602ce3639
  - id: FMrYw7sgyxQ.srt
    parent_uid: d7daadd47b972104cb7537f96c5d8c0f
    technical_location: >-
      https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-ii-inference-limit-theorems/lecture-overview-1/FMrYw7sgyxQ.srt
    title: 3play caption file
    type: null
    uid: a1025e02dbe914527014a0aad69e5039
  - id: FMrYw7sgyxQ.pdf
    parent_uid: d7daadd47b972104cb7537f96c5d8c0f
    technical_location: >-
      https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-ii-inference-limit-theorems/lecture-overview-1/FMrYw7sgyxQ.pdf
    title: 3play pdf file
    type: null
    uid: 1706b709a1c9d599b0bb31feefea7f22
  - id: Caption-3Play YouTube id-SRT
    parent_uid: d7daadd47b972104cb7537f96c5d8c0f
    title: Caption-3Play YouTube id-SRT-English - US
    type: Caption
    uid: 4531548002e4d75d4b83397cf052b0a5
  - id: Transcript-3Play YouTube id-PDF
    parent_uid: d7daadd47b972104cb7537f96c5d8c0f
    title: Transcript-3Play YouTube id-PDF-English - US
    type: Transcript
    uid: a4db6f43eb6f0eb24d825672c956b6c1
  - id: Video-InternetArchive-MP4
    media_location: >-
      https://archive.org/download/MITRES.6-012S18/MITRES6_012S18_L15-01_300k.mp4
    parent_uid: d7daadd47b972104cb7537f96c5d8c0f
    title: Video-Internet Archive-MP4
    type: Video
    uid: 7829479b59a9f684b92f1ec45cdad2f4
inline_embed_id: 50702087lectureoverview54797743
order_index: 1435
parent_uid: b8cdf274e2b0f82662e4cd137e85d308
related_resources_text: ''
short_url: lecture-overview-1
technical_location: >-
  https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-ii-inference-limit-theorems/lecture-overview-1
title: Lecture Overview
transcript: >-
  <p><span m='590'>In the previous lecture, we went through examples of</span>
  <span m='3250'>inference involving some of the variations</span> <span
  m='5830'>of the Bayes rule.</span> </p><p><span m='7590'>One case that we did
  not consider was the case where</span> <span m='10730'>the unknown random
  variable and the observation are both</span> <span
  m='13970'>continuous.</span> </p><p><span m='15520'>In this lecture, we will
  focus exclusively on an important</span> <span m='18740'>model of this
  kind.</span> </p><p><span m='20150'>And because we consider only one specific
  setting, we will</span> <span m='23960'>be able to start it in considerable
  detail.</span> </p><p><span m='28060'>In the model that we consider, we start
  with some basic</span> <span m='30950'>independent normal random
  variables.</span> </p><p><span m='33660'>Some of them, the Theta j, are
  unknown, to be estimated.</span> </p><p><span m='38540'>And some of them, the
  Wi, represent noise.</span> </p><p><span m='42110'>Our observations, Xi, are
  linear functions of these</span> <span m='45900'>basic random
  variables.</span> </p><p><span m='47650'>In particular, since linear functions
  of independent</span> <span m='50810'>normal random variables are normal, the
  observations are</span> <span m='54140'>themselves normal as well.</span>
  </p><p><span m='57450'>This is probably the most commonly used type of model
  in</span> <span m='61920'>all of inference and statistics.</span> </p><p><span
  m='64569'>This is because it is a reasonable approximation in</span> <span
  m='67120'>many situations.</span> </p><p><span m='68510'>Also, it has a very
  clean analytical structure and a</span> <span m='72090'>very simple
  solution.</span> </p><p><span m='74210'>For example, it turns out that the
  posterior distribution of</span> <span m='77860'>each Theta j is itself normal
  and that the MAP and LMS</span> <span m='82890'>estimates coincide.</span>
  </p><p><span m='85110'>This is because the peak of a normal occurs at the
  mean.</span> </p><p><span m='88950'>Furthermore, these estimates are given by
  some simple</span> <span m='92650'>linear functions of the
  observations.</span> </p><p><span m='96550'>We will go over these facts by
  moving through a sequence of</span> <span m='99890'>progressively more complex
  versions.</span> </p><p><span m='102890'>We will start with just one unknown
  and one observation</span> <span m='106700'>and then generalize.</span>
  </p><p><span m='108400'>And we will illustrate the formulation and the
  solution</span> <span m='111350'>through a rather realistic example where we
  estimate the</span> <span m='115050'>trajectory of an object from a few noisy
  measurements.</span> </p><p></p>
uid: d7daadd47b972104cb7537f96c5d8c0f
type: courses
layout: video
---
