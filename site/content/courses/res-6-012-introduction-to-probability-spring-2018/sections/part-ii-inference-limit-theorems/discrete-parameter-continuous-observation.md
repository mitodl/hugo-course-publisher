---
about_this_resource_text: '<p><strong>Instructor:</strong> John Tsitsiklis</p>'
embedded_media:
  - id: Video-YouTube-Stream
    media_location: OlKmZj2TKnk
    parent_uid: 2ffd22273f47edb27e65c77aeb42140d
    title: Video-YouTube-Stream
    type: Video
    uid: 916dde2fd47aeff7159dd28b6eedcdfc
  - id: Thumbnail-YouTube-JPG
    media_location: 'https://img.youtube.com/vi/OlKmZj2TKnk/default.jpg'
    parent_uid: 2ffd22273f47edb27e65c77aeb42140d
    title: Thumbnail-YouTube-JPG
    type: Thumbnail
    uid: 4238bfeb96336c89d601e520dd289cfb
  - id: 3Play-3PlayYouTubeid-MP4
    media_location: OlKmZj2TKnk
    parent_uid: 2ffd22273f47edb27e65c77aeb42140d
    title: 3Play-3Play YouTube id
    type: 3Play
    uid: 4867ae5a67a5415830859e8a6a0baded
  - id: OlKmZj2TKnk.srt
    parent_uid: 2ffd22273f47edb27e65c77aeb42140d
    technical_location: >-
      https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-ii-inference-limit-theorems/discrete-parameter-continuous-observation/OlKmZj2TKnk.srt
    title: 3play caption file
    type: null
    uid: 16a34b9251916acdda430705e00dd3ce
  - id: OlKmZj2TKnk.pdf
    parent_uid: 2ffd22273f47edb27e65c77aeb42140d
    technical_location: >-
      https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-ii-inference-limit-theorems/discrete-parameter-continuous-observation/OlKmZj2TKnk.pdf
    title: 3play pdf file
    type: null
    uid: 96ec2c9ce4d06fd9dc43c1eaaffc3392
  - id: Caption-3Play YouTube id-SRT
    parent_uid: 2ffd22273f47edb27e65c77aeb42140d
    title: Caption-3Play YouTube id-SRT-English - US
    type: Caption
    uid: 0df9dfaf00429fbb250aa042681c2a15
  - id: Transcript-3Play YouTube id-PDF
    parent_uid: 2ffd22273f47edb27e65c77aeb42140d
    title: Transcript-3Play YouTube id-PDF-English - US
    type: Transcript
    uid: ebed3ba463e851e905f9e7cbfe7cefc3
  - id: Video-InternetArchive-MP4
    media_location: >-
      https://archive.org/download/MITRES.6-012S18/MITRES6_012S18_L14-06_300k.mp4
    parent_uid: 2ffd22273f47edb27e65c77aeb42140d
    title: Video-Internet Archive-MP4
    type: Video
    uid: c65803c759bb8c7dcb89769ab9592e72
inline_embed_id: 35834226discreteparametercontinuousobservation70353670
order_index: 1381
parent_uid: b8cdf274e2b0f82662e4cd137e85d308
related_resources_text: ''
short_url: discrete-parameter-continuous-observation
technical_location: >-
  https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-ii-inference-limit-theorems/discrete-parameter-continuous-observation
title: 'Discrete Parameter, Continuous Observation'
transcript: >-
  <p><span m='2080'>In the next variation that we consider,</span> <span
  m='4500'>the random variable Theta is still discrete.</span> </p><p><span
  m='7160'>So it might, for example, represent</span> <span m='9520'>a number of
  alternative hypothesis.</span> </p><p><span m='11900'>But now our observation
  is continuous.</span> </p><p><span m='14640'>Of course, we do have a variation
  of the Bayes rule</span> <span m='17630'>that's applicable to this
  situation.</span> </p><p><span m='20020'>The only difference from the previous
  version of the Bayes</span> <span m='22600'>rule is that now the PMF of X, the
  unconditional</span> <span m='27290'>and the conditional one, is replaced by a
  PDF.</span> </p><p><span m='30590'>Otherwise, everything remains the
  same.</span> </p><p><span m='33610'>A standard example is the
  following.</span> </p><p><span m='37760'>Here we're sending a signal that
  takes one of, let's say,</span> <span m='41550'>three alternative
  values.</span> </p><p><span m='43950'>And what we observe is the signal</span>
  <span m='46300'>that was sent plus some noise.</span> </p><p><span
  m='48710'>And the typical assumption here might</span> <span m='50510'>be that
  the noise has zero mean and a certain variance,</span> <span m='53610'>and is
  independent from the signal that was sent.</span> </p><p><span m='58480'>This
  is an example that we more or less studied some time ago.</span> </p><p><span
  m='63110'>Actually, at that time, we looked at an example</span> <span
  m='65090'>where Theta could only take one out of two values,</span> <span
  m='68230'>but the calculations and the methodology</span> <span
  m='70470'>remains essentially the same as for the case of three values.</span>
  </p><p><span m='74310'>So in principle, we do know at this point</span> <span
  m='76870'>how to apply the Bayes rule in this situation</span> <span
  m='79530'>to come up with a conditional PMF of theta.</span> </p><p><span
  m='83950'>And the key to that calculation was that the term that we
  need,</span> <span m='88330'>the conditional PDF of X, can be obtained from
  this equation</span> <span m='92539'>as follows.</span> </p><p><span
  m='94190'>If I tell you the value of Theta,</span> <span m='96030'>then X is
  essentially the same as W plus a certain constant.</span> </p><p><span
  m='101140'>Adding a constant just shifts the PDF of W</span> <span
  m='105190'>by an amount equal to that constant.</span> </p><p><span
  m='107650'>And, therefore, the conditional PDF of X</span> <span m='110360'>is
  the shifted PDF of the random variable W. Using</span> <span m='114910'>this
  particular fact, we can then apply the Bayes rule,</span> <span
  m='118890'>carry out of the calculations, and suppose that in the end</span>
  <span m='122040'>we came up with these results.</span> </p><p><span
  m='124350'>That is we obtain the specific observation</span> <span
  m='126960'>x and based on that observation, we</span> <span
  m='129280'>calculate the conditional probabilities</span> <span m='131470'>of
  the different choices of Theta.</span> </p><p><span m='133860'>At this point,
  we may use the MAP rule</span> <span m='136670'>and come up with an estimate
  which</span> <span m='139880'>is the value of Theta, which is the more likely
  one.</span> </p><p><span m='144160'>And then we can continue exactly as in the
  case</span> <span m='147590'>of discrete measurements, of discrete
  observations,</span> <span m='151820'>and talk about conditional probabilities
  of error</span> <span m='155050'>and so on.</span> </p><p><span
  m='156270'>Now, the fact that X is continuous</span> <span m='158640'>really
  makes no difference, once we arrive at this picture.</span> </p><p><span
  m='163800'>With the MAP rule we still choose the most likely value</span>
  <span m='166730'>of theta, and this is our estimates.</span> </p><p><span
  m='169290'>And we can calculate the probability</span> <span m='171220'>of
  error, which with the MAP rule</span> <span m='172835'>would be 0.4, exactly
  the same argument</span> <span m='176030'>as for the case of discrete
  observations</span> <span m='178120'>applies and shows that this conditional
  probability</span> <span m='181290'>of error is smallest under the MAP
  rule.</span> </p><p><span m='184550'>And then we can continue similarly</span>
  <span m='186760'>and talk about the overall probability of error, which</span>
  <span m='190010'>can be calculated using the total probability</span> <span
  m='192750'>theorem in two ways.</span> </p><p><span m='195130'>One way is to
  take the conditional probability</span> <span m='197880'>of error for any
  given value of X</span> <span m='201340'>and then average those conditional
  probabilities</span> <span m='203760'>of errors over all the possible choices
  of X.</span> </p><p><span m='207030'>Because X is now continuous, here</span>
  <span m='209010'>we're going to have an integral.</span> </p><p><span
  m='210840'>Alternatively, you can condition on the possible values</span>
  <span m='214350'>of Theta, calculate conditional probabilities of error</span>
  <span m='219090'>for any particular choice of theta,</span> <span
  m='221710'>and then take a weighted average of them.</span> </p><p><span
  m='226560'>In practice, this calculation sometimes</span> <span
  m='229030'>turns out to be the simpler one.</span> </p><p><span
  m='231720'>Finally, we can replicate the argument</span> <span m='233930'>that
  we had in the discrete case.</span> </p><p><span m='235760'>Since the MAP rule
  makes this term here as small as possible,</span> <span m='241560'>it is less
  than or equal to the probability of error</span> <span m='245180'>that you
  would get under any other estimate or estimator,</span> <span m='249740'>then
  it follows that the integral will also</span> <span m='252590'>be as small as
  possible.</span> </p><p><span m='254830'>And therefore, the conclusion is that
  the overall probability</span> <span m='258329'>of error is, again, the
  smallest possible</span> <span m='261660'>when we use the MAP rule.</span>
  </p><p><span m='263970'>And so the MAP rule remains the optimal way</span>
  <span m='267970'>of choosing between alternative hypothesis,</span> <span
  m='271070'>whether X is discrete or continuous.</span> </p><p></p>
uid: 2ffd22273f47edb27e65c77aeb42140d
type: courses
layout: video
---
