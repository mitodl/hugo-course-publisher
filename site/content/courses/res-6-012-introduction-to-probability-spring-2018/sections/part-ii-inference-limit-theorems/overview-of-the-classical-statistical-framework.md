---
about_this_resource_text: '<p><strong>Instructor:</strong> John Tsitsiklis</p>'
embedded_media:
  - id: Video-YouTube-Stream
    media_location: fMHJPEcoC08
    parent_uid: 6208435d9b8108dfae6f074a3f9587c7
    title: Video-YouTube-Stream
    type: Video
    uid: 3f8a10f6a9d13759dede27c64f5505b6
  - id: Thumbnail-YouTube-JPG
    media_location: 'https://img.youtube.com/vi/fMHJPEcoC08/default.jpg'
    parent_uid: 6208435d9b8108dfae6f074a3f9587c7
    title: Thumbnail-YouTube-JPG
    type: Thumbnail
    uid: c260c9fe0f24cf8b580a89d67984781d
  - id: 3Play-3PlayYouTubeid-MP4
    media_location: fMHJPEcoC08
    parent_uid: 6208435d9b8108dfae6f074a3f9587c7
    title: 3Play-3Play YouTube id
    type: 3Play
    uid: 09f480f814c6167efa4ea22eb1994ec3
  - id: fMHJPEcoC08.srt
    parent_uid: 6208435d9b8108dfae6f074a3f9587c7
    technical_location: >-
      https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-ii-inference-limit-theorems/overview-of-the-classical-statistical-framework/fMHJPEcoC08.srt
    title: 3play caption file
    type: null
    uid: 058ef476a7bcbf1445728dcd07ebffd0
  - id: fMHJPEcoC08.pdf
    parent_uid: 6208435d9b8108dfae6f074a3f9587c7
    technical_location: >-
      https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-ii-inference-limit-theorems/overview-of-the-classical-statistical-framework/fMHJPEcoC08.pdf
    title: 3play pdf file
    type: null
    uid: 15297b4cbecaa5cbd507621842da67e4
  - id: Caption-3Play YouTube id-SRT
    parent_uid: 6208435d9b8108dfae6f074a3f9587c7
    title: Caption-3Play YouTube id-SRT-English - US
    type: Caption
    uid: 753f3fef14916dc88433b933bec90ae3
  - id: Transcript-3Play YouTube id-PDF
    parent_uid: 6208435d9b8108dfae6f074a3f9587c7
    title: Transcript-3Play YouTube id-PDF-English - US
    type: Transcript
    uid: 6c633abbac1fa687377b5d4a5d5abf9e
  - id: Video-InternetArchive-MP4
    media_location: >-
      https://archive.org/download/MITRES.6-012S18/MITRES6_012S18_L20-02_300k.mp4
    parent_uid: 6208435d9b8108dfae6f074a3f9587c7
    title: Video-Internet Archive-MP4
    type: Video
    uid: fd340caa8987e4b141e898d40566a489
inline_embed_id: 96945644overviewoftheclassicalstatisticalframework56591255
order_index: 1831
parent_uid: b8cdf274e2b0f82662e4cd137e85d308
related_resources_text: ''
short_url: overview-of-the-classical-statistical-framework
technical_location: >-
  https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-ii-inference-limit-theorems/overview-of-the-classical-statistical-framework
title: Overview of the Classical Statistical Framework
transcript: >-
  <p><span m='560'>In this segment we provide a high level introduction
  into</span> <span m='4310'>the conceptual framework of classical
  statistics.</span> </p><p><span m='8090'>In order to get there, it is better
  to start from what we</span> <span m='11970'>already know and then make a
  comparison.</span> </p><p><span m='14910'>We already know how to make
  inferences by just using the</span> <span m='19430'>Bayes rule.</span>
  </p><p><span m='20560'>In this setting, we have an unknown quantity, theta,
  which</span> <span m='24370'>we model as a random variable.</span>
  </p><p><span m='26270'>And so in particular, it's going to have a
  probability</span> <span m='29480'>distribution.</span> </p><p><span
  m='30800'>And then we make some observations.</span> </p><p><span
  m='33130'>And those observations are modeled as random variables.</span>
  </p><p><span m='36310'>And typically we are given the conditional distribution
  of</span> <span m='39350'>the observations given the unknown variable.</span>
  </p><p><span m='43190'>So these two distributions are the starting points, and
  then</span> <span m='47750'>we do some calculations.</span> </p><p><span
  m='49510'>And we use the Bayes rule.</span> </p><p><span m='51430'>And we find
  the posterior distribution of theta given</span> <span m='54580'>the
  observations.</span> </p><p><span m='56780'>And this tells us all that there
  is to know about the</span> <span m='59860'>unknown quantity, theta, given the
  observations</span> <span m='62460'>that we have made.</span> </p><p><span
  m='64120'>What is important in this framework is that theta is</span> <span
  m='68690'>treated as a random variable.</span> </p><p><span m='71020'>And so
  it has a distribution of its own.</span> </p><p><span m='73930'>And that's our
  starting point.</span> </p><p><span m='76100'>These are our prior beliefs
  about theta before we obtain</span> <span m='80100'>any observations.</span>
  </p><p><span m='83150'>However, one can think of situations where theta
  maybe</span> <span m='87910'>cannot be modeled as a random variable.</span>
  </p><p><span m='90780'>Suppose that theta is at some universal, physical
  constant.</span> </p><p><span m='94940'>For example the mass of the
  electron.</span> </p><p><span m='98690'>Does it make sense to think of that
  quantity as random?</span> </p><p><span m='101590'>And how do we come up with
  a probability distribution for</span> <span m='104600'>that quantity?</span>
  </p><p><span m='105961'>One can argue that in certain situations one should
  not</span> <span m='110050'>think of unknown quantities as being random, but
  rather they</span> <span m='115060'>are just unknown constants.</span>
  </p><p><span m='118610'>They are absolute constants.</span> </p><p><span
  m='120380'>It just happens that we do not know their value.</span>
  </p><p><span m='125690'>Or there may be other situations in which even</span>
  <span m='129460'>though we may think that there is something random
  that</span> <span m='132540'>determines theta, we are reluctant to postulate
  any</span> <span m='136910'>prior distribution.</span> </p><p><span
  m='138190'>We do not want to impose any biases.</span> </p><p><span
  m='141450'>And that leads us to the classic statistical framework</span> <span
  m='144560'>in which unknown quantities are treated as constants, not</span>
  <span m='148970'>as random variables.</span> </p><p><span
  m='150829'>Pictorially the setting is as follows.</span> </p><p><span
  m='153290'>There's an unknown quantity that we wish to estimate.</span>
  </p><p><span m='156260'>And we make some observations, X. Those</span> <span
  m='160290'>observations are random.</span> </p><p><span m='162490'>And they're
  drawn according to a probability distribution.</span> </p><p><span
  m='165840'>And that probability distribution depends, or</span> <span
  m='169630'>rather is affected, by that unknown quantity.</span> </p><p><span
  m='173660'>So for example, for one value of theta, the distribution of</span>
  <span m='178090'>the X's might be this one.</span> </p><p><span m='180100'>And
  for another value of theta, the distribution of the</span> <span
  m='183710'>X's could be a different one.</span> </p><p><span m='186930'>And
  we're trying to guess what theta is.</span> </p><p><span m='190000'>Which in
  some ways is the question, do my data come from</span> <span m='194250'>this
  distribution or do they come from that distribution?</span> </p><p><span
  m='198740'>In order to make a choice of theta, what we do is we take</span>
  <span m='204390'>the data and we process them.</span> </p><p><span
  m='207010'>And after we process them, we come up with our estimate--</span>
  <span m='212340'>or rather estimator.</span> </p><p><span m='216110'>What is
  the estimator?</span> </p><p><span m='217970'>We take the data, and we
  calculate a</span> <span m='221130'>function of the data.</span> </p><p><span
  m='223150'>That's what it means to process the data.</span> </p><p><span
  m='226120'>And that function is our theta hat.</span> </p><p><span
  m='229700'>Now this function, our data processing mechanism, is what</span>
  <span m='233210'>we can call an estimator.</span> </p><p><span m='235550'>But
  quite often, or usually, we also use the same</span> <span
  m='239900'>terminology to call theta hat itself an estimator.</span>
  </p><p><span m='244790'>Now notice that theta hat is a function of the
  random</span> <span m='248570'>variable X. So theta hat is actually a random
  variable.</span> </p><p><span m='254820'>And that's why we denote it with an
  uppercase theta.</span> </p><p><span m='259130'>On the other hand, after you
  obtain some concrete data,</span> <span m='263640'>little x, which are the
  realized values of the random</span> <span m='266850'>variable capital X. Then
  we can apply your estimator to</span> <span m='271060'>that particular input,
  and we compute a specific value--</span> <span m='277790'>call it theta hat
  lower case.</span> </p><p><span m='280390'>And that quantity we call an
  estimate.</span> </p><p><span m='285850'>So this is a useful
  distinction.</span> </p><p><span m='288580'>Always, with random variables, we
  want to distinguish between</span> <span m='291590'>the random variable itself
  indicated by uppercase letters</span> <span m='295030'>and the values of the
  random variable, which are indicated</span> <span m='297780'>with lower case
  letters.</span> </p><p><span m='299430'>Similarly, the estimator is a random
  variable.</span> </p><p><span m='304090'>It's essentially a description of how
  we generate estimates.</span> </p><p><span m='308920'>Whereas the realized
  value, once we have some specific</span> <span m='312770'>observations at
  hand--</span> <span m='314200'>that's what we call an estimate.</span>
  </p><p><span m='318400'>Now let me continue with a few comments.</span>
  </p><p><span m='321280'>The picture, or the setting, that I have here suggests
  that</span> <span m='325780'>X is just one variable and theta is one
  variable.</span> </p><p><span m='329220'>But we can have the same framework,
  even if X and theta</span> <span m='333260'>are multi-dimensional.</span>
  </p><p><span m='334980'>For example, X might consist of several random
  variables.</span> </p><p><span m='339900'>And theta may be a parameter that
  consists of multiple</span> <span m='343280'>components.</span> </p><p><span
  m='345430'>Now you may notice that this notation that we're using here</span>
  <span m='348900'>is a little different from our traditional notation which
  was</span> <span m='353920'>of this form.</span> </p><p><span m='358159'>In
  what ways is it different?</span> </p><p><span m='361470'>The main difference
  is that here, theta is</span> <span m='364700'>not a random variable.</span>
  </p><p><span m='367720'>Theta is just a parameter.</span> </p><p><span
  m='370660'>So what we're dealing with, here, is just an ordinary--</span>
  <span m='376230'>not a conditional distribution.</span> </p><p><span
  m='378570'>It's an ordinary distribution that happens to involve,</span> <span
  m='382950'>inside its description, some parameters theta.</span> </p><p><span
  m='387420'>Just to emphasize the point that these are not conditional</span>
  <span m='390810'>probabilities, because theta is not a random variable,
  we</span> <span m='394570'>use a semicolon instead of using a bar.</span>
  </p><p><span m='398990'>And since theta is not a random variable, we do
  not</span> <span m='402040'>include it in the subscript down here when we talk
  about</span> <span m='406970'>the classical setting.</span> </p><p><span
  m='408830'>The best way to think of the situation mathematically is</span>
  <span m='412590'>that we're essentially dealing with multiple candidate</span>
  <span m='417140'>models, as in this picture.</span> </p><p><span
  m='419250'>This could be one possible model of X. This could be</span> <span
  m='422840'>another possible model of X. We have one such model for</span>
  <span m='427320'>each possible value of theta.</span> </p><p><span
  m='430070'>And if, for example, I were to get data points that sit down</span>
  <span m='435040'>here, then a reasonable way to make an inference could be
  to</span> <span m='439770'>say, these data are extremely unlikely to have
  been</span> <span m='443320'>generated according to this model.</span>
  </p><p><span m='445810'>This data are quite likely to have been</span> <span
  m='447790'>generated by this model.</span> </p><p><span m='449880'>So I'm
  going to pick this particular model.</span> </p><p><span m='452570'>So even
  though we're not treating theta as a random</span> <span m='455770'>variable,
  and we do not have the Bayes rule in our hands--</span> <span m='459380'>we
  can still see, at least from this trivial example,</span> <span
  m='462530'>that there should be a reasonable way of making</span> <span
  m='465200'>inferences.</span> </p><p><span m='467390'>And let me close with
  some comments on the different</span> <span m='470390'>types of problems that
  we may encounter in classical</span> <span m='474110'>statistics.</span>
  </p><p><span m='475420'>One class of problems are so-called hypothesis
  testing</span> <span m='478280'>problems in which we're asked to choose
  between two</span> <span m='481440'>candidate models.</span> </p><p><span
  m='482710'>So the unknown parameter, as in this example, can take one</span>
  <span m='486190'>of two values.</span> </p><p><span m='487540'>So think of a
  machine that produces coins.</span> </p><p><span m='491050'>And coins are
  either fair or they have a</span> <span m='494470'>very specific bias.</span>
  </p><p><span m='496960'>You want to flip the coin, maybe multiple times, and
  then</span> <span m='500700'>decide whether you're dealing with a coin of
  this</span> <span m='502870'>type or of that type.</span> </p><p><span
  m='505290'>There's another type of hypothesis testing problems</span> <span
  m='508340'>which is a little more complicated, for</span> <span
  m='511000'>example this one.</span> </p><p><span m='512429'>We have one
  hypothesis which says that my coin is fair,</span> <span m='517110'>versus an
  alternative hypothesis in</span> <span m='519820'>which my coin is
  unfair.</span> </p><p><span m='522130'>But notice that this hypothesis
  actually includes</span> <span m='525240'>many possible scenarios.</span>
  </p><p><span m='526870'>There are many possible values of theta under which
  this</span> <span m='530300'>hypothesis would be true.</span> </p><p><span
  m='532550'>We will not deal with problems of this kind in this segment,</span>
  <span m='536920'>or in this lecture sequence.</span> </p><p><span
  m='538920'>Instead we will focus exclusively</span> <span m='541200'>on
  estimation problems.</span> </p><p><span m='543280'>In estimation problems,
  the unknown parameter, theta, is</span> <span m='548170'>either continuous or
  can take one of many, many values.</span> </p><p><span m='552940'>What we want
  to do is to design an estimator--</span> <span m='557340'>a way of processing
  the data--</span> <span m='559760'>that comes up with estimates that are
  good.</span> </p><p><span m='564070'>What does it mean that an estimate is
  good?</span> </p><p><span m='566530'>An estimate would be good if the
  resulting value of the</span> <span m='569530'>estimation error--</span> <span
  m='571120'>that is the difference between the estimated value and the</span>
  <span m='574720'>true value--</span> <span m='575500'>if that difference is
  small.</span> </p><p><span m='578200'>You want to keep that difference</span>
  <span m='580480'>small in some sense.</span> </p><p><span m='583160'>Well one
  may need a criterion of what it means to be small.</span> </p><p><span
  m='588010'>And whether we want this in expectation, or with high</span> <span
  m='591600'>probability, and so on.</span> </p><p><span m='593470'>This
  statement, to keep the estimation error small, can be</span> <span
  m='596780'>interpreted in various ways.</span> </p><p><span m='598870'>And
  because of that reason, there's no single approach to</span> <span
  m='602510'>the problem of designing a good estimator.</span> </p><p><span
  m='605590'>And this is something that happens more generally in</span> <span
  m='608040'>classical statistics.</span> </p><p><span m='609710'>Typically
  problems do not admit a single best approach.</span> </p><p><span
  m='615050'>They do not admit unique answers.</span> </p><p><span
  m='617790'>Reasonable people can come up with different methodologies</span>
  <span m='621470'>for approaching the same problem.</span> </p><p><span
  m='623920'>And there is a little bit of an element of an</span> <span
  m='627060'>art involved here.</span> </p><p><span m='629880'>In general, one
  wants to come up with reasonable methods</span> <span m='633230'>that will
  have good properties.</span> </p><p><span m='636120'>And we will see some
  examples of what this may mean.</span> </p><p><span m='639270'>But again, I'm
  emphasizing that there is</span> <span m='641870'>no single best
  method.</span> </p><p><span m='645140'>So whereas the Bayes rule is a
  completely unambiguous way for</span> <span m='649260'>making inferences,
  here, in the context of classical</span> <span m='652720'>statistics, there
  will be some freedom as to what approaches</span> <span m='657020'>one might
  take.</span> </p><p></p>
uid: 6208435d9b8108dfae6f074a3f9587c7
type: course
layout: video
---
