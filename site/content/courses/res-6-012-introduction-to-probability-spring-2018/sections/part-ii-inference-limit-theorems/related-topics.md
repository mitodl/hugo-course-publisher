---
about_this_resource_text: '<p><strong>Instructor:</strong> John Tsitsiklis</p>'
embedded_media:
  - id: Video-YouTube-Stream
    media_location: iBqEF1cB7nE
    parent_uid: 4c54cd4a354e8458576818c98885433b
    title: Video-YouTube-Stream
    type: Video
    uid: 7a3d6dd3a10da941595c1dd532667a17
  - id: Thumbnail-YouTube-JPG
    media_location: 'https://img.youtube.com/vi/iBqEF1cB7nE/default.jpg'
    parent_uid: 4c54cd4a354e8458576818c98885433b
    title: Thumbnail-YouTube-JPG
    type: Thumbnail
    uid: 1291c15c2d010a7f879b7f087207408b
  - id: 3Play-3PlayYouTubeid-MP4
    media_location: iBqEF1cB7nE
    parent_uid: 4c54cd4a354e8458576818c98885433b
    title: 3Play-3Play YouTube id
    type: 3Play
    uid: 7c15ffd1b897623be3676ea22a6095ec
  - id: iBqEF1cB7nE.srt
    parent_uid: 4c54cd4a354e8458576818c98885433b
    technical_location: >-
      https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-ii-inference-limit-theorems/related-topics/iBqEF1cB7nE.srt
    title: 3play caption file
    type: null
    uid: 02e71771cbfac3788752fd11d3f494dc
  - id: iBqEF1cB7nE.pdf
    parent_uid: 4c54cd4a354e8458576818c98885433b
    technical_location: >-
      https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-ii-inference-limit-theorems/related-topics/iBqEF1cB7nE.pdf
    title: 3play pdf file
    type: null
    uid: 8d8a40f319889070ac216c53414ceabc
  - id: Caption-3Play YouTube id-SRT
    parent_uid: 4c54cd4a354e8458576818c98885433b
    title: Caption-3Play YouTube id-SRT-English - US
    type: Caption
    uid: 4ac5d4bc378cab18781abbd6d68da053
  - id: Transcript-3Play YouTube id-PDF
    parent_uid: 4c54cd4a354e8458576818c98885433b
    title: Transcript-3Play YouTube id-PDF-English - US
    type: Transcript
    uid: f1a48991cc4560f9ba59e0cca0da2b65
  - id: Video-InternetArchive-MP4
    media_location: >-
      https://archive.org/download/MITRES.6-012S18/MITRES6_012S18_L18-08_300k.mp4
    parent_uid: 4c54cd4a354e8458576818c98885433b
    title: Video-Internet Archive-MP4
    type: Video
    uid: 147aca64a3ce1d4e097488498797ec5a
inline_embed_id: 66681271relatedtopics54632829
order_index: 1723
parent_uid: b8cdf274e2b0f82662e4cd137e85d308
related_resources_text: ''
short_url: related-topics
technical_location: >-
  https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-ii-inference-limit-theorems/related-topics
title: Related Topics
transcript: >-
  <p><span m='290'>The purpose of this segment is to give you a little bit
  of</span> <span m='3060'>the bigger picture.</span> </p><p><span m='4510'>We
  did discuss some inequalities, we did discuss</span> <span
  m='7750'>convergence of the sample mean--</span> <span m='9820'>that's the
  weak law of large numbers-- and we did discuss a</span> <span
  m='13190'>particular notion of convergence of random</span> <span
  m='15330'>variables, convergence in probability.</span> </p><p><span
  m='17860'>How far can we take those topics?</span> </p><p><span
  m='21510'>Let's start with the issue of inequalities.</span> </p><p><span
  m='23790'>Here, one would like to obtain bounds and approximations on</span>
  <span m='27010'>tail probabilities that are better than the Markov and</span>
  <span m='30620'>Cherbyshev inequalities that we have seen.</span> </p><p><span
  m='33020'>This is indeed possible.</span> </p><p><span m='34740'>For example,
  there is a so-called Chernoff bound that</span> <span m='38300'>takes the
  following form.</span> </p><p><span m='40740'>The Chernoff bound tells us that
  the probability that the</span> <span m='44630'>sample mean is away from the
  true mean by at least a, where</span> <span m='51030'>a is a positive number,
  this probability is bounded above</span> <span m='55710'>by a function that
  falls exponentially with n and where</span> <span m='60290'>the exponent
  depends on the particular number, a, that we</span> <span m='63700'>are
  considering.</span> </p><p><span m='64860'>But in any case, this term in the
  exponent</span> <span m='67110'>is a positive quantity.</span> </p><p><span
  m='69510'>Notice that this is much better, much stronger than</span> <span
  m='74100'>what we obtained from the Cherbyshev inequality because</span> <span
  m='77900'>in the Cherbyshev inequality, we only obtain an inequality</span>
  <span m='81560'>for this probability that falls off as the</span> <span
  m='84470'>rate of 1 over n.</span> </p><p><span m='86950'>So this falls much
  faster, and so it tells us that this</span> <span m='89830'>probability is
  indeed much smaller than what the</span> <span m='93550'>Cherbyshev inequality
  might predict.</span> </p><p><span m='96100'>However, this inequality requires
  some additional</span> <span m='98860'>assumptions on the random variables
  involved.</span> </p><p><span m='102259'>Another type of approximation on this
  tail probability can</span> <span m='105880'>be obtained through the central
  limit theorem, which</span> <span m='109120'>will actually be the next
  topic</span> <span m='111060'>that we will be studying.</span> </p><p><span
  m='112960'>Very loosely speaking, the central limit theorem tells us</span>
  <span m='117660'>that the random variable M sub n, which is the sample
  mean,</span> <span m='123530'>behaves as if it were a normal random variable
  with the mean</span> <span m='129910'>and the variance that it should
  have.</span> </p><p><span m='133380'>We know that this is the mean and the
  variance of the sample</span> <span m='136650'>mean, but the central limit
  theorem tells us that in</span> <span m='139840'>addition to that, we can also
  pretend that the sample mean</span> <span m='143800'>is normal and carry out
  approximations as if this were</span> <span m='147870'>a normal random
  variable.</span> </p><p><span m='150020'>Now, this statement that I'm making
  here is</span> <span m='152640'>only a loose statement.</span> </p><p><span
  m='154660'>It is not mathematically completely accurate.</span> </p><p><span
  m='158550'>We will see later a more accurate statement of the</span> <span
  m='161579'>central limit theorem.</span> </p><p><span m='163640'>In a
  different direction, we can talk about different types</span> <span
  m='167090'>of convergence.</span> </p><p><span m='168750'>We did define
  convergence in probability, but that's not</span> <span m='172550'>the only
  notion of convergence that's</span> <span m='174490'>relevant to random
  variables.</span> </p><p><span m='176350'>There's an alternative notion, which
  is convergence with</span> <span m='179160'>probability one.</span>
  </p><p><span m='180770'>Here is what it means.</span> </p><p><span
  m='183240'>We have a single probabilistic experiment.</span> </p><p><span
  m='186240'>And within that the experiment, we have a sequence</span> <span
  m='189290'>of random variables and another random variable, and</span> <span
  m='195220'>we want to talk about this random variable converging to</span>
  <span m='198160'>that random variable.</span> </p><p><span m='200360'>What do
  we mean by that?</span> </p><p><span m='202280'>We consider a typical outcome
  of the experiment, that is,</span> <span m='207040'>some omega.</span>
  </p><p><span m='209110'>Look at the values of the random variable Yn under
  that</span> <span m='213730'>particular omega, and look at that sequence of
  values, the</span> <span m='218530'>values of the different random variables
  under that</span> <span m='220890'>particular outcome.</span> </p><p><span
  m='222740'>Under that particular outcome, Y also has a certain
  numerical</span> <span m='227010'>value, and we're interested in whether this
  convergence takes</span> <span m='233070'>place as n goes to infinity.</span>
  </p><p><span m='236760'>Now for some outcomes, omega, this will happen.</span>
  </p><p><span m='239520'>For some, it will not happen.</span> </p><p><span
  m='242080'>We will say that we have convergence with probability</span> <span
  m='244730'>one if this event has probability equal to 1.</span> </p><p><span
  m='253870'>That is, there is probability one, that is, essential</span> <span
  m='258060'>certainty, that when an outcome of the experiment is</span> <span
  m='261760'>obtained, the resulting sequence of values of the</span> <span
  m='265380'>random variables Yn will converge to the value of the</span> <span
  m='268730'>random variable Y.</span> </p><p><span m='270980'>Now, this
  definition is easy to write down, but to actually</span> <span
  m='275010'>understand what it really means and the ways it is</span> <span
  m='277670'>different from convergence in probability is not so easy.</span>
  </p><p><span m='281770'>It does take some conceptual effort, and we will
  not</span> <span m='285050'>discuss it any further at this point.</span>
  </p><p><span m='287880'>Let me just say that this is a stronger notion
  of</span> <span m='290670'>convergence.</span> </p><p><span m='291659'>If you
  have convergence with probability one, you also gets</span> <span
  m='294710'>convergence in probability.</span> </p><p><span m='297080'>And it
  turns out that the law of large numbers also holds</span> <span
  m='302880'>under this stronger notion of convergence.</span> </p><p><span
  m='306280'>That is, we have that the sample mean converges to the</span> <span
  m='310420'>true mean with probability one.</span> </p><p><span m='314170'>This
  is the so-called strong law of large numbers, and</span> <span
  m='317970'>because this is a stronger notion of convergence, a more</span>
  <span m='321050'>demanding one, that's why this is called the strong
  law.</span> </p><p><span m='326320'>Incidentally, at this point, you might be
  quite uncertain</span> <span m='330490'>and confused as to what is really the
  difference between</span> <span m='334470'>these two notions of
  convergence.</span> </p><p><span m='337040'>The definitions do look different,
  but what is the</span> <span m='340470'>real difference?</span> </p><p><span
  m='341900'>This is quite subtle, and it does take</span> <span
  m='344770'>quite a bit of thinking.</span> </p><p><span m='346270'>It's not
  supposed to be something that is obvious.</span> </p><p><span m='350090'>So
  the purpose of this discussion is only to point</span> <span m='353190'>out
  these further directions but without, at this point,</span> <span
  m='357480'>going into it in any depth.</span> </p><p><span m='359890'>Finally,
  there is another notion of convergence in which</span> <span m='363730'>we're
  looking at the distributions of the random</span> <span m='366460'>variables
  involved.</span> </p><p><span m='368070'>So we may have a sequence of random
  variables.</span> </p><p><span m='370790'>Each one of them has a certain
  distribution described by a</span> <span m='373770'>CDF, and we can ask the
  question, does this sequence</span> <span m='377680'>of CDFs converge to a
  limiting CDF?</span> </p><p><span m='381510'>If that happens, then we say that
  we have convergence in</span> <span m='384800'>distribution, and this is more
  or less the type of</span> <span m='388450'>convergence that shows up when we
  deal with the central limit</span> <span m='392220'>theorem because this is
  really a statement about</span> <span m='395040'>distributions, that the
  distribution of the sample</span> <span m='397560'>mean in some sense starts
  to approach the distribution of a</span> <span m='401570'>normal random
  variable.</span> </p><p></p>
uid: 4c54cd4a354e8458576818c98885433b
type: course
layout: video
---
