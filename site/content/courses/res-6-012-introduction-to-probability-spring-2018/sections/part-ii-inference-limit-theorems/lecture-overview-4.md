---
about_this_resource_text: '<p><strong>Instructor:</strong> John Tsitsiklis</p>'
embedded_media:
  - id: Video-YouTube-Stream
    media_location: sD0i6bWxmRY
    parent_uid: 50335b6b63e4391187115d5ab5b802fc
    title: Video-YouTube-Stream
    type: Video
    uid: 4d855ea700e503a9ba58a3ebfb71ac7c
  - id: Thumbnail-YouTube-JPG
    media_location: 'https://img.youtube.com/vi/sD0i6bWxmRY/default.jpg'
    parent_uid: 50335b6b63e4391187115d5ab5b802fc
    title: Thumbnail-YouTube-JPG
    type: Thumbnail
    uid: bb459a168d9eeb40bffbf6c56167a57f
  - id: 3Play-3PlayYouTubeid-MP4
    media_location: sD0i6bWxmRY
    parent_uid: 50335b6b63e4391187115d5ab5b802fc
    title: 3Play-3Play YouTube id
    type: 3Play
    uid: fcecd47738a35535fd4c40b82643e605
  - id: sD0i6bWxmRY.srt
    parent_uid: 50335b6b63e4391187115d5ab5b802fc
    technical_location: >-
      https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-ii-inference-limit-theorems/lecture-overview-4/sD0i6bWxmRY.srt
    title: 3play caption file
    type: null
    uid: 24145e35754ddcab47ffe18812b28038
  - id: sD0i6bWxmRY.pdf
    parent_uid: 50335b6b63e4391187115d5ab5b802fc
    technical_location: >-
      https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-ii-inference-limit-theorems/lecture-overview-4/sD0i6bWxmRY.pdf
    title: 3play pdf file
    type: null
    uid: 44bfad7f692a73fe19f77ff78e1112f7
  - id: Caption-3Play YouTube id-SRT
    parent_uid: 50335b6b63e4391187115d5ab5b802fc
    title: Caption-3Play YouTube id-SRT-English - US
    type: Caption
    uid: af49be8dbbbc0cbb9344efa32584d261
  - id: Transcript-3Play YouTube id-PDF
    parent_uid: 50335b6b63e4391187115d5ab5b802fc
    title: Transcript-3Play YouTube id-PDF-English - US
    type: Transcript
    uid: fb33f5a34e67a9d078c21b85965d8df0
  - id: Video-InternetArchive-MP4
    media_location: >-
      https://archive.org/download/MITRES.6-012S18/MITRES6_012S18_L18-01_300k.mp4
    parent_uid: 50335b6b63e4391187115d5ab5b802fc
    title: Video-Internet Archive-MP4
    type: Video
    uid: 33f675c1ed1491ddde4678a5ecb593e3
inline_embed_id: 80339389lectureoverview16753568
order_index: 1660
parent_uid: b8cdf274e2b0f82662e4cd137e85d308
related_resources_text: ''
short_url: lecture-overview-4
technical_location: >-
  https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-ii-inference-limit-theorems/lecture-overview-4
title: Lecture Overview
transcript: >-
  <p><span m='500'>In this lecture, we develop the weak law of large
  numbers.</span> </p><p><span m='5160'>Loosely speaking, the weak law of large
  numbers says that if</span> <span m='9060'>we have a sequence of independent
  random variables</span> <span m='11950'>with the same distribution, then the
  average of these</span> <span m='15670'>random variables, which is called the
  sample mean,</span> <span m='18940'>approaches the expected value of the
  distribution.</span> </p><p><span m='22560'>In this sense, it reinforces our
  interpretation of the</span> <span m='26380'>expected value as some kind of
  overall average.</span> </p><p><span m='31660'>The weak law of large numbers
  is the</span> <span m='33550'>reason why polling works.</span> </p><p><span
  m='36040'>By asking many people about the value of some attribute,</span>
  <span m='39360'>and by taking the average of the responses, we can get
  a</span> <span m='43020'>good estimate of the average over the entire
  population.</span> </p><p><span m='48730'>On the mathematical side, in order
  to derive the weak law</span> <span m='51990'>of large numbers, we will first
  need to develop some</span> <span m='55160'>inequalities, namely the Markov
  and Chebyshev</span> <span m='58600'>inequalities.</span> </p><p><span
  m='60150'>Both of them tell us something about tail probabilities.</span>
  </p><p><span m='64870'>Suppose that a is a number.</span> </p><p><span
  m='67140'>Then it is reasonable to expect that the probability</span> <span
  m='70400'>that the random variable exceeds a will be small when a</span> <span
  m='75080'>is very large.</span> </p><p><span m='76660'>But how small?</span>
  </p><p><span m='78320'>The Markov and Chebyshev inequalities give us
  some</span> <span m='81000'>answers to this question, based only on knowledge
  of the</span> <span m='84710'>mean and the variance of the
  distribution.</span> </p><p><span m='88780'>Finally, we will have to deal with
  a technical issue.</span> </p><p><span m='92200'>The weak law of large numbers
  talks about the convergence of</span> <span m='95810'>a random variable to a
  number.</span> </p><p><span m='98390'>For this to make sense, we need to
  define an appropriate</span> <span m='101780'>notion of convergence.</span>
  </p><p><span m='103740'>We will introduce one such notion that goes under
  the</span> <span m='106910'>name of convergence in probability.</span>
  </p><p><span m='110050'>And we will see that in many respects, it is similar
  to the</span> <span m='113520'>common notion of convergence of numbers.</span>
  </p><p></p>
uid: 50335b6b63e4391187115d5ab5b802fc
type: courses
layout: video
---
