---
about_this_resource_text: '<p><strong>Instructor:</strong> John Tsitsiklis</p>'
embedded_media:
  - id: Video-YouTube-Stream
    media_location: aNLEnFtWwhg
    parent_uid: 58fb04ff51ba259d644509e915a36716
    title: Video-YouTube-Stream
    type: Video
    uid: d84c8a60e3428c011696be9db10c6b16
  - id: Thumbnail-YouTube-JPG
    media_location: 'https://img.youtube.com/vi/aNLEnFtWwhg/default.jpg'
    parent_uid: 58fb04ff51ba259d644509e915a36716
    title: Thumbnail-YouTube-JPG
    type: Thumbnail
    uid: 53e200ba8009a933852d28cfa1b73561
  - id: 3Play-3PlayYouTubeid-MP4
    media_location: aNLEnFtWwhg
    parent_uid: 58fb04ff51ba259d644509e915a36716
    title: 3Play-3Play YouTube id
    type: 3Play
    uid: 02da6d3c195ea9ea220442de0bffc40a
  - id: aNLEnFtWwhg.srt
    parent_uid: 58fb04ff51ba259d644509e915a36716
    technical_location: >-
      https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-ii-inference-limit-theorems/lecture-overview/aNLEnFtWwhg.srt
    title: 3play caption file
    type: null
    uid: 5414a626c38a35764597878276af683f
  - id: aNLEnFtWwhg.pdf
    parent_uid: 58fb04ff51ba259d644509e915a36716
    technical_location: >-
      https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-ii-inference-limit-theorems/lecture-overview/aNLEnFtWwhg.pdf
    title: 3play pdf file
    type: null
    uid: 9e85adb6541344309ea466a1c2c5e403
  - id: Caption-3Play YouTube id-SRT
    parent_uid: 58fb04ff51ba259d644509e915a36716
    title: Caption-3Play YouTube id-SRT-English - US
    type: Caption
    uid: e5d46ce81773dfd6d84aa945b3ba726a
  - id: Transcript-3Play YouTube id-PDF
    parent_uid: 58fb04ff51ba259d644509e915a36716
    title: Transcript-3Play YouTube id-PDF-English - US
    type: Transcript
    uid: 570c7410571479883eb263fb8e52bb49
  - id: Video-InternetArchive-MP4
    media_location: >-
      https://archive.org/download/MITRES.6-012S18/MITRES6_012S18_L14-01_300k.mp4
    parent_uid: 58fb04ff51ba259d644509e915a36716
    title: Video-Internet Archive-MP4
    type: Video
    uid: f37f5114d1f2968305b90abfa60bc269
inline_embed_id: 88577720lectureoverview9902475
order_index: 1336
parent_uid: b8cdf274e2b0f82662e4cd137e85d308
related_resources_text: ''
short_url: lecture-overview
technical_location: >-
  https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-ii-inference-limit-theorems/lecture-overview
title: Lecture Overview
transcript: >-
  <p><span m='750'>In this lecture, we start our systematic</span> <span
  m='3010'>study of Bayesian inference.</span> </p><p><span m='5290'>We will
  first talk a little bit about the big picture,</span> <span m='8340'>about
  inference in general, the huge range of possible</span> <span
  m='11620'>applications, and the different types of problems</span> <span
  m='14260'>that one may encounter.</span> </p><p><span m='16079'>For example,
  we have hypothesis testing problems in</span> <span m='18990'>which we are
  trying to choose between a finite and usually</span> <span m='22130'>small
  number of alternative hypotheses or estimation</span> <span m='26280'>problems
  where we want to estimate as close as we can an</span> <span m='30160'>unknown
  numerical quantity.</span> </p><p><span m='33020'>We then move into the</span>
  <span m='34030'>specifics of Bayesian inference.</span> </p><p><span
  m='36170'>The central idea is that we always use the Bayes rule to</span>
  <span m='39090'>find the posterior distribution of an unknown</span> <span
  m='41770'>random variable based on observations of a related</span> <span
  m='45160'>random variable.</span> </p><p><span m='46740'>Depending on whether
  the random variables are discrete</span> <span m='49330'>or continuous, we
  must of course you use the appropriate</span> <span m='52160'>version of the
  Bayes rule.</span> </p><p><span m='55060'>If we want to summarize the
  posterior in a single number,</span> <span m='58670'>that is, to come up with
  a numerical estimate of the</span> <span m='61420'>unknown random variable, we
  then have some options.</span> </p><p><span m='65160'>One is to report the
  value at which the</span> <span m='67860'>posterior is largest.</span>
  </p><p><span m='69850'>Another is to report the mean of the conditional</span>
  <span m='72610'>distribution.</span> </p><p><span m='74000'>These go under the
  acronyms MAP and LMS.</span> </p><p><span m='77710'>We will see shortly what
  these acronyms stand for.</span> </p><p><span m='82700'>Given any particular
  method for coming up with a point</span> <span m='85590'>estimate, there are
  certain performance metrics that tell</span> <span m='89280'>us how good the
  estimate is.</span> </p><p><span m='91680'>For hypothesis testing problems,
  the appropriate</span> <span m='94509'>metric is the probability of error, the
  probability of</span> <span m='97850'>making a mistake.</span> </p><p><span
  m='100100'>For problems of estimating a numerical quantity, an</span> <span
  m='103670'>appropriate metric that we will be using a lot is the</span> <span
  m='106509'>expected value of the squared error.</span> </p><p><span
  m='110420'>As we will see, there will be no new mathematics in this</span>
  <span m='113440'>lecture, just a few definitions, a few new terms,</span>
  <span m='117380'>and an application of the Bayes rule.</span> </p><p><span
  m='119759'>Nevertheless, it is important to be able to apply the Bayes</span>
  <span m='123240'>rule systematically and with confidence.</span> </p><p><span
  m='126290'>For this reason, we will be going over several examples.</span>
  </p><p></p>
uid: 58fb04ff51ba259d644509e915a36716
type: courses
layout: video
---
